#!/bin/sh
#BSUB -J spark-ngs
#BSUB -W 2:00 # requesting 10 minutes
#BSUB -o spark-ngs-%J.log # output extra o means overwrite
#BSUB -n 20 # requesting 96 cores
#BSUB -R "span[hosts=-1] rusage[mem=8000]"

#module load new
#module load java
#module load open_mpi

#source setup_environment.sh

# setup the spark paths
export SPARK_HOME=$HOME/spark

# custom python installation path
#export PATH=$HOME/miniconda/bin:$PATH

# initialize the nodes -- run ./start_spark_lsf.py -h to see other options
export SPARK_LOCAL_DIRS="$TMPDIR, /scratch/spark-${USER}, /scratch/spark"
export LOCAL_DIRS="$TMPDIR, /scratch/spark-${USER}, /scratch/spark"
export SPARK_WORKER_DIR="$TMPDIR/work, /scratch/spark-${USER}/work, /scratch/spark/work"

./start_spark_lsf.py 24 8

