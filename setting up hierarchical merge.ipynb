{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test hierarchical merge in FOF algorithm\n",
    "\n",
    "#### the idea is this: \n",
    "\n",
    "* after the local FOF stage, each partition reports the particles it holds in the overlap region\n",
    "* do a reduceByKey or treeAggregate of some sort to collect the groups belonging to the same particles\n",
    "* produce a mapping of $G -> G_1$ and distribute to all hosts in form of broadcast lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.setrecursionlimit(sys.getrecursionlimit()*10)\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# import matplotlib.pylab as plt\n",
    "# %matplotlib inline\n",
    "# import matplotlib.patches as patches\n",
    "# plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "import line_profiler\n",
    "\n",
    "from Cython.Compiler.Options import directive_defaults\n",
    "\n",
    "directive_defaults['linetrace'] = True\n",
    "directive_defaults['binding'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spark_fof\n",
    "import spark_fof_c\n",
    "from fof import fof\n",
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_rectangle(rec, ax=None):\n",
    "    if ax is None: \n",
    "        ax = plt.subplot(aspect='equal')\n",
    "    \n",
    "    if isinstance(rec, (list, tuple)):\n",
    "        for r in rec: \n",
    "            plot_rectangle(r,ax)\n",
    "    \n",
    "    else:\n",
    "        size = (rec.maxes-rec.mins)\n",
    "        ax.add_patch(patches.Rectangle(rec.mins, size[0], size[1], fill=False, zorder=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SPARK_CONF_DIR'] = './conf'\n",
    "os.environ['SPARK_DRIVER_MEMORY'] = '4G'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rok/miniconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import pynbody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x11798ffd0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = SparkConf()\n",
    "\n",
    "conf.set('spark.python.profile', 'true')\n",
    "conf.set('spark.executor.memory', '3G')\n",
    "conf.set('spark.driver.memory', '4G')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(master='local[4]', conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.addPyFile('spark_fof.py')\n",
    "sc.addPyFile('spark_fof_c.pyx')\n",
    "sc.addPyFile('spark_fof_c.c')\n",
    "sc.addPyFile('spark_fof_c.so')\n",
    "sc.addPyFile('fof.so')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 1\n",
    "tau = 7.8125e-4\n",
    "mins = np.array([-.5,-.5,-.5])\n",
    "maxs= np.array([.5,.5,.5])\n",
    "nMinMembers = 8\n",
    "fof_analyzer = spark_fof.FOFAnalyzer(sc, '/Users/rok/polybox/euclid256.nat_no_header', nMinMembers, N, tau, mins, maxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.21 s, sys: 73.5 ms, total: 2.28 s\n",
      "Wall time: 1min 50s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "105330"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time len(fof_analyzer.groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Profile of RDD<id=2>\n",
      "============================================================\n",
      "         74304 function calls (74268 primitive calls) in 3.043 seconds\n",
      "\n",
      "   Ordered by: internal time, cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "     8210    2.749    0.000    2.749    0.000 {method 'read' of 'file' objects}\n",
      "     4096    0.149    0.000    0.196    0.000 spark_tipsy.py:31(convert_to_fof_particle)\n",
      "     4096    0.036    0.000    0.036    0.000 {numpy.core.multiarray.zeros}\n",
      "     4114    0.027    0.000    3.026    0.001 spark_tipsy.py:38(convert_to_fof_particle_partition)\n",
      "     4114    0.018    0.000    2.784    0.001 serializers.py:155(_read_with_length)\n",
      "     4114    0.013    0.000    3.039    0.001 rdd.py:1004(<genexpr>)\n",
      "     4114    0.010    0.000    2.598    0.001 serializers.py:542(read_int)\n",
      "     4096    0.010    0.000    0.010    0.000 {numpy.core.multiarray.frombuffer}\n",
      "     4096    0.007    0.000    0.009    0.000 spark_tipsy.py:12(addInPlace)\n",
      "     4096    0.006    0.000    0.015    0.000 accumulators.py:160(add)\n",
      "     4114    0.005    0.000    2.788    0.001 serializers.py:136(load_stream)\n",
      "     4114    0.004    0.000    0.004    0.000 {_struct.unpack}\n",
      "    12324    0.002    0.000    0.002    0.000 {len}\n",
      "       36    0.002    0.000    3.041    0.084 {sum}\n",
      "     4096    0.001    0.000    0.001    0.000 {method 'iteritems' of 'dict' objects}\n",
      "     4096    0.001    0.000    0.001    0.000 serializers.py:332(loads)\n",
      "       18    0.001    0.000    0.001    0.000 serializers.py:259(dump_stream)\n",
      "    54/18    0.000    0.000    3.041    0.169 rdd.py:2345(pipeline_func)\n",
      "       18    0.000    0.000    0.000    0.000 {cPickle.dumps}\n",
      "       54    0.000    0.000    3.041    0.056 rdd.py:316(func)\n",
      "       18    0.000    0.000    3.043    0.169 worker.py:104(process)\n",
      "       18    0.000    0.000    0.000    0.000 serializers.py:414(dumps)\n",
      "       18    0.000    0.000    0.000    0.000 serializers.py:549(write_int)\n",
      "       36    0.000    0.000    0.000    0.000 {method 'write' of 'file' objects}\n",
      "       36    0.000    0.000    0.000    0.000 rdd.py:861(func)\n",
      "       18    0.000    0.000    0.000    0.000 rdd.py:995(<lambda>)\n",
      "       18    0.000    0.000    3.041    0.169 rdd.py:1004(<lambda>)\n",
      "       18    0.000    0.000    0.000    0.000 {_struct.pack}\n",
      "       18    0.000    0.000    0.000    0.000 {operator.add}\n",
      "       18    0.000    0.000    0.000    0.000 {iter}\n",
      "       18    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "\n",
      "\n",
      "============================================================\n",
      "Profile of RDD<id=3>\n",
      "============================================================\n",
      "         1950999 function calls (1938675 primitive calls) in 21.590 seconds\n",
      "\n",
      "   Ordered by: internal time, cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "     4096   14.446    0.004   16.324    0.004 spark_fof_c.pyx:92(new_partitioning_cython)\n",
      "      676    1.340    0.002    1.340    0.002 {method 'write' of 'file' objects}\n",
      "     4096    1.062    0.000    1.062    0.000 spark_fof_c.pyx:186(ghost_mask)\n",
      "      169    0.820    0.005    0.820    0.005 {cPickle.dumps}\n",
      "     4114    0.805    0.000    1.740    0.000 spark_tipsy.py:45(set_particle_IDs_partition)\n",
      "24576/12288    0.690    0.000    1.158    0.000 _internal.py:423(_dtype_from_pep3118)\n",
      "     8210    0.509    0.000    0.509    0.000 {method 'read' of 'file' objects}\n",
      "   212992    0.209    0.000    0.209    0.000 stringsource:341(__cinit__)\n",
      "     4114    0.160    0.000    0.160    0.000 {range}\n",
      "     4096    0.155    0.000    0.202    0.000 spark_tipsy.py:31(convert_to_fof_particle)\n",
      "    12288    0.143    0.000    0.195    0.000 _internal.py:22(_makenames_list)\n",
      "    16384    0.104    0.000    1.263    0.000 {numpy.core.multiarray.array}\n",
      "      356    0.103    0.000   20.227    0.057 rdd.py:1697(add_shuffle_key)\n",
      "     4096    0.096    0.000    0.096    0.000 {method 'sort' of 'numpy.ndarray' objects}\n",
      "    12288    0.082    0.000    0.277    0.000 _internal.py:55(_usefields)\n",
      "     4096    0.076    0.000    0.236    0.000 arraysetops.py:96(unique)\n",
      "   200704    0.061    0.000    0.264    0.000 stringsource:643(memoryview_cwrapper)\n",
      "    61440    0.058    0.000    0.058    0.000 _internal.py:436(get_dummy_name)\n",
      "     4096    0.042    0.000    0.042    0.000 {method 'nonzero' of 'numpy.ndarray' objects}\n",
      "   212992    0.042    0.000    0.042    0.000 stringsource:368(__dealloc__)\n",
      "   110592    0.039    0.000    0.039    0.000 _internal.py:624(_gcd)\n",
      "     4096    0.039    0.000    0.039    0.000 {numpy.core.multiarray.zeros}\n",
      "     4096    0.033    0.000    0.033    0.000 {method 'flatten' of 'numpy.ndarray' objects}\n",
      "     5137    0.030    0.000   19.280    0.004 spark_fof.py:113(partition_wrapper)\n",
      "    12288    0.026    0.000    0.033    0.000 {method 'sort' of 'list' objects}\n",
      "    12288    0.023    0.000    0.023    0.000 {map}\n",
      "     4096    0.022    0.000    0.022    0.000 {numpy.core.multiarray.concatenate}\n",
      "     4096    0.022    0.000    1.152    0.000 {spark_fof_c.ghost_mask}\n",
      "    98304    0.021    0.000    0.021    0.000 {method 'startswith' of 'str' objects}\n",
      "     4114    0.021    0.000    0.773    0.000 spark_tipsy.py:38(convert_to_fof_particle_partition)\n",
      "       18    0.020    0.001   21.589    1.199 serializers.py:132(dump_stream)\n",
      "     4096    0.019    0.000    0.023    0.000 _internal.py:277(_newnames)\n",
      "    12288    0.018    0.000    0.025    0.000 stringsource:985(memoryview_fromslice)\n",
      "     4114    0.018    0.000    0.542    0.000 serializers.py:155(_read_with_length)\n",
      "     4096    0.017    0.000    0.435    0.000 fromnumeric.py:43(_wrapit)\n",
      "    61440    0.017    0.000    0.017    0.000 {method 'index' of 'str' objects}\n",
      "     4096    0.017    0.000    0.452    0.000 fromnumeric.py:1490(nonzero)\n",
      "    62463    0.016    0.000    0.016    0.000 {isinstance}\n",
      "   200704    0.016    0.000    0.016    0.000 stringsource:649(memoryview_check)\n",
      "     4096    0.013    0.000   16.358    0.004 {spark_fof_c.new_partitioning_cython}\n",
      "    12288    0.011    0.000    1.273    0.000 numeric.py:414(asarray)\n",
      "     5119    0.010    0.000    0.021    0.000 rdd.py:61(portable_hash)\n",
      "    90979    0.010    0.000    0.010    0.000 {len}\n",
      "    98304    0.010    0.000    0.010    0.000 {method 'get' of 'dict' objects}\n",
      "   110592    0.009    0.000    0.009    0.000 {method 'isdigit' of 'str' objects}\n",
      "     4114    0.009    0.000    0.020    0.000 serializers.py:542(read_int)\n",
      "     4096    0.008    0.000    1.073    0.000 spark_fof_c.pyx:186(ghost_mask (wrapper))\n",
      "     4096    0.008    0.000    0.008    0.000 {numpy.core.multiarray.frombuffer}\n",
      "     4114    0.007    0.000    0.550    0.000 serializers.py:136(load_stream)\n",
      "    49152    0.007    0.000    0.007    0.000 _internal.py:44(<lambda>)\n",
      "     4096    0.007    0.000    0.008    0.000 numeric.py:484(asanyarray)\n",
      "    54271    0.007    0.000    0.007    0.000 {method 'append' of 'list' objects}\n",
      "    12288    0.005    0.000    0.005    0.000 {method 'split' of 'str' objects}\n",
      "    12288    0.005    0.000    0.005    0.000 _internal.py:433(next_dummy_name)\n",
      "     4114    0.004    0.000    0.004    0.000 {_struct.unpack}\n",
      "    12343    0.004    0.000    0.004    0.000 {method 'keys' of 'dict' objects}\n",
      "     4096    0.002    0.000    0.002    0.000 {method 'remove' of 'list' objects}\n",
      "     4096    0.002    0.000    0.002    0.000 {getattr}\n",
      "      338    0.002    0.000    1.343    0.004 serializers.py:143(_write_with_length)\n",
      "    12288    0.002    0.000    0.002    0.000 stringsource:507(__getbuffer__)\n",
      "    12288    0.001    0.000    0.001    0.000 stringsource:962(__dealloc__)\n",
      "    12288    0.001    0.000    0.001    0.000 stringsource:545(__get__)\n",
      "     4096    0.001    0.000    0.001    0.000 serializers.py:332(loads)\n",
      "      507    0.001    0.000    0.001    0.000 {_struct.pack}\n",
      "     5119    0.001    0.000    0.001    0.000 {hash}\n",
      "      338    0.001    0.000    0.002    0.000 serializers.py:549(write_int)\n",
      "      169    0.000    0.000    0.821    0.005 serializers.py:414(dumps)\n",
      "        4    0.000    0.000    0.000    0.000 {open}\n",
      "       18    0.000    0.000   21.590    1.199 worker.py:104(process)\n",
      "      169    0.000    0.000    0.001    0.000 serializers.py:538(pack_long)\n",
      "        4    0.000    0.000    0.000    0.000 {cPickle.load}\n",
      "        4    0.000    0.000    0.001    0.000 broadcast.py:82(load)\n",
      "    54/18    0.000    0.000    0.000    0.000 rdd.py:2345(pipeline_func)\n",
      "      338    0.000    0.000    0.000    0.000 serializers.py:335(dumps)\n",
      "       18    0.000    0.000    0.001    0.000 broadcast.py:92(value)\n",
      "       18    0.000    0.000    0.000    0.000 {hasattr}\n",
      "       18    0.000    0.000    0.000    0.000 rdd.py:316(func)\n",
      "       18    0.000    0.000    0.000    0.000 {min}\n",
      "       18    0.000    0.000    0.000    0.000 {sum}\n",
      "       18    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
      "       18    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        4    0.000    0.000    0.000    0.000 {gc.disable}\n",
      "        4    0.000    0.000    0.000    0.000 {gc.enable}\n",
      "\n",
      "\n",
      "============================================================\n",
      "Profile of RDD<id=7>\n",
      "============================================================\n",
      "         38450 function calls in 24.987 seconds\n",
      "\n",
      "   Ordered by: internal time, cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        8   21.698    2.712   21.698    2.712 {fof.fof.run}\n",
      "      346    0.920    0.003    0.920    0.003 {method 'read' of 'file' objects}\n",
      "       68    0.668    0.010    0.668    0.010 {cPickle.dumps}\n",
      "      169    0.631    0.004    0.631    0.004 {cPickle.loads}\n",
      "      136    0.356    0.003    0.356    0.003 {method 'write' of 'file' objects}\n",
      "        8    0.354    0.044    0.354    0.044 {numpy.core.multiarray.concatenate}\n",
      "       76    0.104    0.001   23.856    0.314 spark_fof.py:135(run_local_fof)\n",
      "        8    0.101    0.013   24.983    3.123 serializers.py:259(dump_stream)\n",
      "        8    0.066    0.008    0.066    0.008 spark_fof_c.pyx:206(encode_gid (wrapper))\n",
      "      177    0.052    0.000    1.608    0.009 serializers.py:136(load_stream)\n",
      "     5119    0.011    0.000    0.017    0.000 shape_base.py:9(atleast_1d)\n",
      "        8    0.006    0.001    1.987    0.248 shape_base.py:232(hstack)\n",
      "        8    0.004    0.001   24.987    3.123 worker.py:104(process)\n",
      "     5119    0.003    0.000    0.004    0.000 numeric.py:484(asanyarray)\n",
      "      177    0.002    0.000    1.556    0.009 serializers.py:155(_read_with_length)\n",
      "     5119    0.001    0.000    0.001    0.000 rdd.py:1540(<lambda>)\n",
      "    10575    0.001    0.000    0.001    0.000 {len}\n",
      "      177    0.001    0.000    0.569    0.003 serializers.py:542(read_int)\n",
      "     5119    0.001    0.000    0.001    0.000 {numpy.core.multiarray.array}\n",
      "     5187    0.001    0.000    0.001    0.000 {method 'append' of 'list' objects}\n",
      "      169    0.001    0.000    0.632    0.004 serializers.py:421(loads)\n",
      "       68    0.000    0.000    0.001    0.000 serializers.py:549(write_int)\n",
      "      177    0.000    0.000    0.000    0.000 {_struct.unpack}\n",
      "       68    0.000    0.000    0.668    0.010 serializers.py:414(dumps)\n",
      "       68    0.000    0.000    0.000    0.000 {_struct.pack}\n",
      "        8    0.000    0.000    0.000    0.000 shape_base.py:378(array_split)\n",
      "        8    0.000    0.000    0.000    0.000 serializers.py:217(load_stream)\n",
      "        8    0.000    0.000    0.000    0.000 stringsource:341(__cinit__)\n",
      "        8    0.000    0.000    0.066    0.008 {spark_fof_c.encode_gid}\n",
      "       76    0.000    0.000    0.000    0.000 fromnumeric.py:457(swapaxes)\n",
      "       76    0.000    0.000    0.000    0.000 {method 'swapaxes' of 'numpy.ndarray' objects}\n",
      "        8    0.000    0.000    0.000    0.000 shape_base.py:427(split)\n",
      "       16    0.000    0.000    0.000    0.000 {range}\n",
      "        8    0.000    0.000    0.000    0.000 stringsource:643(memoryview_cwrapper)\n",
      "        8    0.000    0.000    0.000    0.000 rdd.py:2345(pipeline_func)\n",
      "        8    0.000    0.000    0.000    0.000 serializers.py:220(_load_stream_without_unbatching)\n",
      "        8    0.000    0.000    0.000    0.000 stringsource:368(__dealloc__)\n",
      "        8    0.000    0.000    0.000    0.000 rdd.py:288(func)\n",
      "        8    0.000    0.000    0.000    0.000 {built-in method from_iterable}\n",
      "        8    0.000    0.000    0.000    0.000 spark_fof.py:150(<lambda>)\n",
      "        8    0.000    0.000    0.000    0.000 stringsource:649(memoryview_check)\n",
      "        8    0.000    0.000    0.000    0.000 {iter}\n",
      "        8    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "\n",
      "\n",
      "============================================================\n",
      "Profile of RDD<id=8>\n",
      "============================================================\n",
      "         2031695 function calls (2031679 primitive calls) in 3.215 seconds\n",
      "\n",
      "   Ordered by: internal time, cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "      760    1.040    0.001    1.040    0.001 {cPickle.dumps}\n",
      "       68    0.359    0.005    0.359    0.005 {cPickle.loads}\n",
      "   118316    0.253    0.000    0.552    0.000 copy.py:145(deepcopy)\n",
      "      144    0.210    0.001    0.210    0.001 {method 'read' of 'file' objects}\n",
      "        8    0.194    0.024    1.856    0.232 shuffle.py:229(mergeValues)\n",
      "   118316    0.185    0.000    0.185    0.000 spark_fof.py:408(pid_gid)\n",
      "   118316    0.182    0.000    0.199    0.000 copy.py:267(_keep_alive)\n",
      "     1528    0.133    0.000    1.305    0.001 rdd.py:1697(add_shuffle_key)\n",
      "       68    0.113    0.002    0.113    0.002 {numpy.core.multiarray.where}\n",
      "   118316    0.073    0.000    0.116    0.000 rdd.py:61(portable_hash)\n",
      "   118316    0.070    0.000    0.622    0.000 rdd.py:1800(createZero)\n",
      "   118316    0.069    0.000    0.736    0.000 rdd.py:1804(<lambda>)\n",
      "   118316    0.058    0.000    0.066    0.000 copy.py:226(_deepcopy_list)\n",
      "   118316    0.045    0.000    0.045    0.000 spark_fof.py:183(<lambda>)\n",
      "       76    0.042    0.001    0.614    0.008 serializers.py:136(load_stream)\n",
      "   473288    0.035    0.000    0.035    0.000 {id}\n",
      "   118328    0.034    0.000    0.034    0.000 {isinstance}\n",
      "   236632    0.024    0.000    0.024    0.000 {method 'get' of 'dict' objects}\n",
      "       16    0.019    0.001    0.019    0.001 {method 'items' of 'dict' objects}\n",
      "        8    0.015    0.002    1.335    0.167 serializers.py:132(dump_stream)\n",
      "       68    0.012    0.000    0.125    0.002 spark_fof.py:181(<lambda>)\n",
      "   118316    0.011    0.000    0.011    0.000 {method 'append' of 'list' objects}\n",
      "     3040    0.008    0.000    0.008    0.000 {method 'write' of 'file' objects}\n",
      "   118316    0.008    0.000    0.008    0.000 {hash}\n",
      "     1520    0.004    0.000    0.015    0.000 serializers.py:143(_write_with_length)\n",
      "        8    0.003    0.000    1.880    0.235 rdd.py:316(func)\n",
      "     2280    0.002    0.000    0.002    0.000 {_struct.pack}\n",
      "     1520    0.002    0.000    0.005    0.000 serializers.py:549(write_int)\n",
      "       76    0.001    0.000    0.572    0.008 serializers.py:155(_read_with_length)\n",
      "      149    0.001    0.000    0.001    0.000 {psutil._psutil_osx.proc_memory_info}\n",
      "      149    0.001    0.000    0.005    0.000 shuffle.py:37(get_used_memory)\n",
      "      760    0.001    0.000    1.041    0.001 serializers.py:414(dumps)\n",
      "      149    0.001    0.000    0.002    0.000 _psosx.py:267(memory_info)\n",
      "     3907    0.001    0.000    0.001    0.000 {len}\n",
      "      760    0.000    0.000    0.001    0.000 serializers.py:538(pack_long)\n",
      "        8    0.000    0.000    0.001    0.000 shuffle.py:69(_get_local_dirs)\n",
      "      149    0.000    0.000    0.003    0.000 __init__.py:936(memory_info)\n",
      "       68    0.000    0.000    0.360    0.005 serializers.py:421(loads)\n",
      "       76    0.000    0.000    0.001    0.000 serializers.py:542(read_int)\n",
      "      153    0.000    0.000    0.003    0.000 _psosx.py:196(wrapper)\n",
      "      149    0.000    0.000    0.000    0.000 <string>:8(__new__)\n",
      "     1520    0.000    0.000    0.000    0.000 serializers.py:335(dumps)\n",
      "      149    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x100186920}\n",
      "        4    0.000    0.000    0.000    0.000 {psutil._psutil_osx.proc_create_time}\n",
      "        8    0.000    0.000    1.876    0.235 rdd.py:1774(combineLocally)\n",
      "        8    0.000    0.000    0.001    0.000 shuffle.py:196(__init__)\n",
      "       76    0.000    0.000    0.000    0.000 {_struct.unpack}\n",
      "        4    0.000    0.000    0.000    0.000 __init__.py:344(_init)\n",
      "      149    0.000    0.000    0.000    0.000 {hasattr}\n",
      "       24    0.000    0.000    0.000    0.000 posixpath.py:61(join)\n",
      "        8    0.000    0.000    0.019    0.002 shuffle.py:337(items)\n",
      "     24/8    0.000    0.000    1.880    0.235 rdd.py:2345(pipeline_func)\n",
      "        8    0.000    0.000    0.000    0.000 shuffle.py:134(_compressed_serializer)\n",
      "        8    0.000    0.000    0.000    0.000 random.py:277(shuffle)\n",
      "      181    0.000    0.000    0.000    0.000 {posix.getpid}\n",
      "        8    0.000    0.000    3.215    0.402 worker.py:104(process)\n",
      "       87    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}\n",
      "        8    0.000    0.000    0.000    0.000 {function seed at 0x10237e0c8}\n",
      "        8    0.000    0.000    0.000    0.000 random.py:100(seed)\n",
      "        8    0.000    0.000    0.000    0.000 UserDict.py:91(get)\n",
      "        8    0.000    0.000    0.000    0.000 serializers.py:217(load_stream)\n",
      "        4    0.000    0.000    0.000    0.000 _psosx.py:282(create_time)\n",
      "        8    0.000    0.000    0.000    0.000 serializers.py:481(__init__)\n",
      "       16    0.000    0.000    0.000    0.000 serializers.py:127(__init__)\n",
      "        8    0.000    0.000    0.000    0.000 UserDict.py:35(__getitem__)\n",
      "        8    0.000    0.000    0.000    0.000 serializers.py:255(__init__)\n",
      "       72    0.000    0.000    0.000    0.000 {method 'startswith' of 'str' objects}\n",
      "        8    0.000    0.000    0.000    0.000 rdd.py:303(func)\n",
      "        8    0.000    0.000    0.000    0.000 random.py:91(__init__)\n",
      "       72    0.000    0.000    0.000    0.000 {method 'endswith' of 'str' objects}\n",
      "        8    0.000    0.000    0.000    0.000 {min}\n",
      "        4    0.000    0.000    0.000    0.000 __init__.py:341(__init__)\n",
      "       16    0.000    0.000    0.000    0.000 {method 'random' of '_random.Random' objects}\n",
      "        8    0.000    0.000    0.000    0.000 {method 'split' of 'str' objects}\n",
      "        8    0.000    0.000    0.000    0.000 UserDict.py:103(__contains__)\n",
      "        8    0.000    0.000    0.000    0.000 serializers.py:220(_load_stream_without_unbatching)\n",
      "        8    0.000    0.000    0.000    0.000 shuffle.py:118(__init__)\n",
      "        4    0.000    0.000    0.000    0.000 __init__.py:619(create_time)\n",
      "        8    0.000    0.000    0.000    0.000 {iter}\n",
      "       16    0.000    0.000    0.000    0.000 {built-in method from_iterable}\n",
      "        4    0.000    0.000    0.000    0.000 _psosx.py:221(__init__)\n",
      "        8    0.000    0.000    0.000    0.000 serializers.py:190(__init__)\n",
      "        8    0.000    0.000    0.000    0.000 rdd.py:288(func)\n",
      "        4    0.000    0.000    0.000    0.000 __init__.py:503(pid)\n",
      "        8    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "\n",
      "\n",
      "============================================================\n",
      "Profile of RDD<id=12>\n",
      "============================================================\n",
      "         363780 function calls (363772 primitive calls) in 1.058 seconds\n",
      "\n",
      "   Ordered by: internal time, cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "      104    0.444    0.004    0.444    0.004 {cPickle.dumps}\n",
      "      760    0.203    0.000    0.203    0.000 {cPickle.loads}\n",
      "        8    0.107    0.013    0.446    0.056 shuffle.py:263(mergeCombiners)\n",
      "        8    0.089    0.011    0.594    0.074 serializers.py:259(dump_stream)\n",
      "     1528    0.071    0.000    0.071    0.000 {method 'read' of 'file' objects}\n",
      "    76183    0.040    0.000    0.040    0.000 spark_fof.py:185(<lambda>)\n",
      "    42133    0.024    0.000    0.024    0.000 {sorted}\n",
      "    42133    0.023    0.000    0.047    0.000 spark_fof.py:183(<lambda>)\n",
      "    76183    0.018    0.000    0.018    0.000 rdd.py:1540(<lambda>)\n",
      "        8    0.016    0.002    0.016    0.002 {method 'items' of 'dict' objects}\n",
      "   118316    0.011    0.000    0.011    0.000 shuffle.py:257(_object_size)\n",
      "      768    0.002    0.000    0.278    0.000 serializers.py:155(_read_with_length)\n",
      "        8    0.002    0.000    0.464    0.058 rdd.py:316(func)\n",
      "      208    0.002    0.000    0.002    0.000 {method 'write' of 'file' objects}\n",
      "      768    0.001    0.000    0.002    0.000 serializers.py:542(read_int)\n",
      "      768    0.001    0.000    0.279    0.000 serializers.py:136(load_stream)\n",
      "      760    0.001    0.000    0.204    0.000 serializers.py:421(loads)\n",
      "       64    0.001    0.000    0.002    0.000 shuffle.py:37(get_used_memory)\n",
      "       64    0.000    0.000    0.000    0.000 {psutil._psutil_osx.proc_memory_info}\n",
      "      768    0.000    0.000    0.000    0.000 {_struct.unpack}\n",
      "       64    0.000    0.000    0.001    0.000 _psosx.py:267(memory_info)\n",
      "      104    0.000    0.000    0.001    0.000 serializers.py:549(write_int)\n",
      "        8    0.000    0.000    0.001    0.000 shuffle.py:69(_get_local_dirs)\n",
      "      104    0.000    0.000    0.444    0.004 serializers.py:414(dumps)\n",
      "      104    0.000    0.000    0.000    0.000 {_struct.pack}\n",
      "       64    0.000    0.000    0.001    0.000 __init__.py:936(memory_info)\n",
      "      984    0.000    0.000    0.000    0.000 {len}\n",
      "       64    0.000    0.000    0.000    0.000 <string>:8(__new__)\n",
      "       64    0.000    0.000    0.001    0.000 _psosx.py:196(wrapper)\n",
      "       24    0.000    0.000    0.000    0.000 posixpath.py:61(join)\n",
      "        8    0.000    0.000    0.001    0.000 shuffle.py:196(__init__)\n",
      "        8    0.000    0.000    0.016    0.002 shuffle.py:337(items)\n",
      "        8    0.000    0.000    1.058    0.132 worker.py:104(process)\n",
      "        8    0.000    0.000    0.000    0.000 random.py:277(shuffle)\n",
      "       64    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x100186920}\n",
      "        8    0.000    0.000    0.463    0.058 rdd.py:1782(_mergeCombiners)\n",
      "        8    0.000    0.000    0.000    0.000 {function seed at 0x10237e0c8}\n",
      "        8    0.000    0.000    0.000    0.000 random.py:100(seed)\n",
      "     16/8    0.000    0.000    0.464    0.058 rdd.py:2345(pipeline_func)\n",
      "       64    0.000    0.000    0.000    0.000 {hasattr}\n",
      "        8    0.000    0.000    0.000    0.000 shuffle.py:134(_compressed_serializer)\n",
      "        8    0.000    0.000    0.000    0.000 rdd.py:303(func)\n",
      "        8    0.000    0.000    0.000    0.000 serializers.py:217(load_stream)\n",
      "       96    0.000    0.000    0.000    0.000 {posix.getpid}\n",
      "        8    0.000    0.000    0.000    0.000 UserDict.py:91(get)\n",
      "       16    0.000    0.000    0.000    0.000 serializers.py:127(__init__)\n",
      "       72    0.000    0.000    0.000    0.000 {method 'startswith' of 'str' objects}\n",
      "        8    0.000    0.000    0.000    0.000 serializers.py:481(__init__)\n",
      "        8    0.000    0.000    0.000    0.000 rdd.py:288(func)\n",
      "        8    0.000    0.000    0.000    0.000 serializers.py:255(__init__)\n",
      "       72    0.000    0.000    0.000    0.000 {method 'endswith' of 'str' objects}\n",
      "        8    0.000    0.000    0.000    0.000 random.py:91(__init__)\n",
      "        8    0.000    0.000    0.000    0.000 {method 'split' of 'str' objects}\n",
      "       16    0.000    0.000    0.000    0.000 {method 'random' of '_random.Random' objects}\n",
      "        8    0.000    0.000    0.000    0.000 serializers.py:220(_load_stream_without_unbatching)\n",
      "        8    0.000    0.000    0.000    0.000 UserDict.py:103(__contains__)\n",
      "       16    0.000    0.000    0.000    0.000 {built-in method from_iterable}\n",
      "        8    0.000    0.000    0.000    0.000 serializers.py:190(__init__)\n",
      "        8    0.000    0.000    0.000    0.000 UserDict.py:35(__getitem__)\n",
      "        8    0.000    0.000    0.000    0.000 shuffle.py:118(__init__)\n",
      "       16    0.000    0.000    0.000    0.000 {iter}\n",
      "        8    0.000    0.000    0.000    0.000 {isinstance}\n",
      "       24    0.000    0.000    0.000    0.000 {id}\n",
      "        8    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "\n",
      "\n",
      "============================================================\n",
      "Profile of RDD<id=13>\n",
      "============================================================\n",
      "         2560 function calls (2552 primitive calls) in 3.898 seconds\n",
      "\n",
      "   Ordered by: internal time, cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "       68    0.928    0.014    0.928    0.014 spark_fof_c.pyx:72(remap_gid_partition_cython)\n",
      "        8    0.789    0.099    0.789    0.099 {cPickle.dumps}\n",
      "       68    0.484    0.007    0.484    0.007 {cPickle.loads}\n",
      "       68    0.469    0.007    0.469    0.007 {method 'sort' of 'numpy.ndarray' objects}\n",
      "        4    0.387    0.097    0.387    0.097 {cPickle.load}\n",
      "      144    0.308    0.002    0.308    0.002 {method 'read' of 'file' objects}\n",
      "       16    0.175    0.011    2.693    0.168 spark_fof.py:285(partition_count_groups)\n",
      "       68    0.083    0.001    0.707    0.010 arraysetops.py:96(unique)\n",
      "       68    0.081    0.001    0.081    0.001 {method 'flatten' of 'numpy.ndarray' objects}\n",
      "       76    0.067    0.001    0.861    0.011 serializers.py:136(load_stream)\n",
      "       68    0.041    0.001    0.041    0.001 {method 'nonzero' of 'numpy.ndarray' objects}\n",
      "      136    0.022    0.000    0.022    0.000 {numpy.core.multiarray.concatenate}\n",
      "       68    0.015    0.000    0.725    0.011 spark_fof.py:280(count_groups)\n",
      "        4    0.013    0.003    0.401    0.100 broadcast.py:82(load)\n",
      "       68    0.010    0.000    0.010    0.000 function_base.py:1507(diff)\n",
      "        8    0.007    0.001    3.497    0.437 serializers.py:259(dump_stream)\n",
      "        8    0.005    0.001    0.005    0.001 {_functools.reduce}\n",
      "       68    0.004    0.000    0.004    0.000 {numpy.core.multiarray.where}\n",
      "       16    0.002    0.000    0.002    0.000 {method 'write' of 'file' objects}\n",
      "       76    0.002    0.000    0.795    0.010 serializers.py:155(_read_with_length)\n",
      "       68    0.001    0.000    0.001    0.000 stringsource:341(__cinit__)\n",
      "        4    0.001    0.000    0.001    0.000 {open}\n",
      "       76    0.001    0.000    1.793    0.024 spark_fof.py:256(remap_partition)\n",
      "       68    0.001    0.000    0.931    0.014 {spark_fof_c.remap_gid_partition_cython}\n",
      "      136    0.001    0.000    0.001    0.000 numeric.py:484(asanyarray)\n",
      "       68    0.000    0.000    0.485    0.007 serializers.py:421(loads)\n",
      "        8    0.000    0.000    3.898    0.487 worker.py:104(process)\n",
      "       76    0.000    0.000    0.003    0.000 serializers.py:542(read_int)\n",
      "       68    0.000    0.000    0.929    0.014 spark_fof_c.pyx:72(remap_gid_partition_cython (wrapper))\n",
      "       68    0.000    0.000    0.041    0.001 fromnumeric.py:1490(nonzero)\n",
      "       68    0.000    0.000    0.001    0.000 stringsource:643(memoryview_cwrapper)\n",
      "      220    0.000    0.000    0.000    0.000 {len}\n",
      "       76    0.000    0.000    0.000    0.000 {_struct.unpack}\n",
      "      136    0.000    0.000    0.000    0.000 {numpy.core.multiarray.array}\n",
      "        8    0.000    0.000    0.000    0.000 serializers.py:549(write_int)\n",
      "       68    0.000    0.000    0.000    0.000 stringsource:368(__dealloc__)\n",
      "       16    0.000    0.000    2.698    0.169 rdd.py:789(func)\n",
      "       24    0.000    0.000    0.401    0.017 rdd.py:316(func)\n",
      "        8    0.000    0.000    0.401    0.050 broadcast.py:92(value)\n",
      "       68    0.000    0.000    0.000    0.000 stringsource:649(memoryview_check)\n",
      "        8    0.000    0.000    0.789    0.099 serializers.py:414(dumps)\n",
      "        8    0.000    0.000    0.000    0.000 serializers.py:217(load_stream)\n",
      "     16/8    0.000    0.000    0.401    0.050 rdd.py:2345(pipeline_func)\n",
      "        8    0.000    0.000    0.401    0.050 spark_fof.py:264(<lambda>)\n",
      "        8    0.000    0.000    0.000    0.000 {_struct.pack}\n",
      "        8    0.000    0.000    0.000    0.000 {hasattr}\n",
      "        8    0.000    0.000    0.000    0.000 spark_fof.py:308(<lambda>)\n",
      "        8    0.000    0.000    0.000    0.000 serializers.py:220(_load_stream_without_unbatching)\n",
      "        8    0.000    0.000    2.693    0.337 {next}\n",
      "        8    0.000    0.000    0.000    0.000 {built-in method from_iterable}\n",
      "       16    0.000    0.000    0.000    0.000 {iter}\n",
      "        4    0.000    0.000    0.000    0.000 {gc.enable}\n",
      "        8    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        4    0.000    0.000    0.000    0.000 {gc.disable}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc.show_profiles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that the results make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ps = np.concatenate(fof_analyzer.particle_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert(len(ps) == len(pynbody.load('/Users/rok/polybox/euclid256.nat')))\n",
    "n_groups = fof.run(ps, tau, nMinMembers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of groups to 8 particle = 105761\n",
      "number of groups via spark-fof = 105330\n"
     ]
    }
   ],
   "source": [
    "print 'number of groups to %d particle = %d'%(nMinMembers, n_groups)\n",
    "print 'number of groups via spark-fof = %d'%(len(fof_analyzer.groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ps_fof = np.concatenate(fof_analyzer.final_fof_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16777216"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16819349"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ps_fof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fof_analyzer.merged_rdd.map(lambda p: p[np.where(not p['is_ghost'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%cython \n",
    "\n",
    "def filter_ghostsa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
