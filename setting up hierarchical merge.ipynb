{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a simple synthetic dataset to test hierarchical merge in FOF algorithm\n",
    "\n",
    "#### the idea is this: \n",
    "\n",
    "* after the local FOF stage, each partition reports the particles it holds in the overlap region\n",
    "* do a reduceByKey or treeAggregate of some sort to collect the groups belonging to the same particles\n",
    "* produce a mapping of $G -> G_1$ and distribute to all hosts in form of broadcast lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rok/miniconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.setrecursionlimit(sys.getrecursionlimit()*10)\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spark_fof\n",
    "import spark_fof_c\n",
    "import fof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_rectangle(rec, ax=None):\n",
    "    if ax is None: \n",
    "        ax = plt.subplot(aspect='equal')\n",
    "    \n",
    "    if isinstance(rec, (list, tuple)):\n",
    "        for r in rec: \n",
    "            plot_rectangle(r,ax)\n",
    "    \n",
    "    else:\n",
    "        size = (rec.maxes-rec.mins)\n",
    "        ax.add_patch(patches.Rectangle(rec.mins, size[0], size[1], fill=False, zorder=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create the arrays\n",
    "from spark_fof_c import pdt\n",
    "pdt_tipsy = np.dtype({'names': (\"mass\", \"x\", \"y\", \"z\", \"vx\", \"vy\", \"vz\", \"eps\", \"phi\"),\n",
    "                                  'formats': ('f','f','f','f','f','f','f','f','f')})\n",
    "\n",
    "pdt_tipsy = np.dtype([('mass', 'f4'),('pos', 'f4', 3),('vel', 'f4', 3), ('eps', 'f4'), ('phi', 'f4')])\n",
    "# nps = 1000000\n",
    "# ngs = 1\n",
    "# particles = np.zeros(nps, dtype=pdt)\n",
    "# done_ps = 0\n",
    "# #centers = np.random.rand(ngs,3)*1.7 - 0.85\n",
    "# centers = np.array([0,0,0]).reshape(1,3)\n",
    "# for group, center in zip(range(ngs), centers): \n",
    "#     print group, center\n",
    "#     group_ps = nps/ngs\n",
    "#     if nps - (done_ps + group_ps) < group_ps:\n",
    "#         group_ps = nps - done_ps \n",
    "#     particles['pos'][done_ps:done_ps+group_ps] = \\\n",
    "#         np.random.multivariate_normal(center, [[.5,0,0],[0,.5,0],[0,0,.5]], group_ps)\n",
    "#     done_ps += group_ps\n",
    "   \n",
    "# particles['iOrder'] = range(nps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from spark_fof_c import pdt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SPARK_CONF_DIR'] = './conf'\n",
    "os.environ['SPARK_DRIVER_MEMORY'] = '4G'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import pynbody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf = SparkConf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x11613e6d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.set('spark.python.profile', 'true')\n",
    "conf.set('spark.executor.memory', '3G')\n",
    "conf.set('spark.driver.memory', '4G')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(master='local[*]', conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.addPyFile('spark_fof.py')\n",
    "sc.addPyFile('spark_util.py')\n",
    "sc.addPyFile('spark_fof_c.pyx')\n",
    "sc.addPyFile('spark_fof_c.c')\n",
    "sc.addPyFile('spark_fof_c.so')\n",
    "sc.addPyFile('fof.so')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 2\n",
    "tau = 7.8125e-4\n",
    "mins = np.array([-.5,-.5,-.5], dtype=np.float)\n",
    "maxs= np.array([.5,.5,.5], dtype=np.float)\n",
    "domain_containers = spark_fof.setup_domain(N,tau,maxs,mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# f, ax = plt.subplots(subplot_kw={'aspect':'equal'}, figsize=(15,15))\n",
    "# pynbody.plot.image(s.d, width=1, units = 'Msol Mpc^-2', cmap=plt.cm.Greys, show_cbar=False, subplot=ax)\n",
    "# #plot_rectangle(domain_containers[0].bufferRectangle, ax=ax)\n",
    "# for p in particles[::1000000]: \n",
    "#     plot_rectangle(domain_containers[spark_fof.get_bin_cython(p['pos'],2**N, np.array(mins), np.array(maxs))], ax=ax)\n",
    "#     plot_rectangle(domain_containers[spark_fof.get_bin_cython(p['pos'], 2**N, np.array(mins),np.array(maxs))].bufferRectangle, ax=ax)\n",
    "#     ax.plot(p['pos'][0], p['pos'][1], '.')\n",
    "# plt.draw()\n",
    "# ax.set_xlim(-.5,.5)\n",
    "# ax.set_ylim(-.5,.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the base RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_fof_particle(s): \n",
    "    p_arr = np.frombuffer(s, pdt_tipsy)\n",
    "    \n",
    "    new_arr = np.zeros(len(p_arr), dtype=pdt)\n",
    "    new_arr['pos'] = p_arr['pos']\n",
    "    \n",
    "    return new_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.accumulators import AccumulatorParam\n",
    "\n",
    "class dictAdd(AccumulatorParam):\n",
    "    def zero(self, value):\n",
    "        return {i:0 for i in range(len(value))}\n",
    "    def addInPlace(self, val1, val2): \n",
    "        for k, v in val2.iteritems(): \n",
    "            val1[k] += v\n",
    "        return val1\n",
    "    \n",
    "def read_tipsy_output(filename, chunksize = 2048): \n",
    "    \"\"\"\n",
    "    Read a tipsy file and set the sequential particle IDs\n",
    "    \n",
    "    This scans through the data twice -- first to get partition particle counts\n",
    "    and a second time to actually set the particle IDs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # helper functions\n",
    "    def convert_to_fof_particle_partition(index, iterator): \n",
    "        for s in iterator: \n",
    "            a = convert_to_fof_particle(s)\n",
    "            if count: \n",
    "                npart_acc.add({index: len(a)})\n",
    "            yield a\n",
    "\n",
    "    def set_particle_IDs_partition(index, iterator): \n",
    "        p_counts = partition_counts.value\n",
    "        local_index = 0\n",
    "        start_index = sum([p_counts[i] for i in range(index)])\n",
    "        for arr in iterator:\n",
    "            arr['iOrder'] = range(start_index + local_index, start_index + local_index + len(arr))\n",
    "            local_index += len(arr)\n",
    "            yield arr\n",
    "    \n",
    "    rec_rdd = sc.binaryRecords(filename, pdt_tipsy.itemsize*chunksize)\n",
    "    nPartitions = rec_rdd.getNumPartitions()\n",
    "    # set the partition count accumulator\n",
    "    npart_acc = sc.accumulator({i:0 for i in range(nPartitions)}, dictAdd())\n",
    "    count=True\n",
    "    # read the data and count the particles per partition\n",
    "    rec_rdd = rec_rdd.mapPartitionsWithIndex(convert_to_fof_particle_partition)\n",
    "    rec_rdd.count()\n",
    "    count=False\n",
    "\n",
    "    partition_counts = sc.broadcast(npart_acc.value)\n",
    "\n",
    "    return rec_rdd.mapPartitionsWithIndex(set_particle_IDs_partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_rdd = read_tipsy_output('/Users/rok/polybox/euclid256.nat_no_header')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8192"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.show_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ps = np.concatenate(p_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16777216"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of groups to 1 particle = 7251094\n",
      "CPU times: user 18.3 s, sys: 64.1 ms, total: 18.3 s\n",
      "Wall time: 18.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "nMinMembers = 1\n",
    "n_groups = fof.run(ps, tau, nMinMembers)\n",
    "print 'number of groups to %d particle = %d'%(nMinMembers, n_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Profile of RDD<id=94>\n",
      "============================================================\n",
      "         148032 function calls (147996 primitive calls) in 1.279 seconds\n",
      "\n",
      "   Ordered by: internal time, cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "    16402    0.636    0.000    0.636    0.000 {method 'read' of 'file' objects}\n",
      "     8192    0.320    0.000    0.426    0.000 <ipython-input-15-d9bd1d33c88e>:1(convert_to_fof_particle)\n",
      "     8192    0.081    0.000    0.081    0.000 {numpy.core.multiarray.zeros}\n",
      "     8210    0.051    0.000    1.263    0.000 <ipython-input-139-69ff1337d1a3>:20(convert_to_fof_particle_partition)\n",
      "     8210    0.046    0.000    0.722    0.000 serializers.py:155(_read_with_length)\n",
      "     8210    0.027    0.000    0.081    0.000 serializers.py:542(read_int)\n",
      "     8192    0.025    0.000    0.028    0.000 <ipython-input-139-69ff1337d1a3>:6(addInPlace)\n",
      "     8192    0.023    0.000    0.023    0.000 {numpy.core.multiarray.frombuffer}\n",
      "     8192    0.022    0.000    0.050    0.000 accumulators.py:160(add)\n",
      "     8210    0.013    0.000    0.735    0.000 serializers.py:136(load_stream)\n",
      "     8210    0.010    0.000    1.274    0.000 rdd.py:1004(<genexpr>)\n",
      "     8210    0.008    0.000    0.008    0.000 {_struct.unpack}\n",
      "    24612    0.007    0.000    0.007    0.000 {len}\n",
      "     8192    0.003    0.000    0.003    0.000 {method 'iteritems' of 'dict' objects}\n",
      "       36    0.003    0.000    1.277    0.035 {sum}\n",
      "     8192    0.003    0.000    0.003    0.000 serializers.py:332(loads)\n",
      "       18    0.001    0.000    0.002    0.000 serializers.py:259(dump_stream)\n",
      "       18    0.000    0.000    0.000    0.000 {cPickle.dumps}\n",
      "       18    0.000    0.000    1.279    0.071 worker.py:104(process)\n",
      "    54/18    0.000    0.000    1.277    0.071 rdd.py:2345(pipeline_func)\n",
      "       36    0.000    0.000    0.000    0.000 rdd.py:861(func)\n",
      "       54    0.000    0.000    1.277    0.024 rdd.py:316(func)\n",
      "       18    0.000    0.000    0.000    0.000 serializers.py:549(write_int)\n",
      "       36    0.000    0.000    0.000    0.000 {method 'write' of 'file' objects}\n",
      "       18    0.000    0.000    0.000    0.000 rdd.py:995(<lambda>)\n",
      "       18    0.000    0.000    1.277    0.071 rdd.py:1004(<lambda>)\n",
      "       18    0.000    0.000    0.000    0.000 serializers.py:414(dumps)\n",
      "       18    0.000    0.000    0.000    0.000 {_struct.pack}\n",
      "       18    0.000    0.000    0.000    0.000 {operator.add}\n",
      "       18    0.000    0.000    0.000    0.000 {iter}\n",
      "       18    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "\n",
      "\n",
      "============================================================\n",
      "Profile of RDD<id=95>\n",
      "============================================================\n",
      "         205128 function calls in 11.650 seconds\n",
      "\n",
      "   Ordered by: internal time, cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "    16384    5.185    0.000    5.185    0.000 {method 'write' of 'file' objects}\n",
      "     8210    2.323    0.000    4.342    0.001 <ipython-input-139-69ff1337d1a3>:27(set_particle_IDs_partition)\n",
      "     8192    1.723    0.000    1.723    0.000 {cPickle.dumps}\n",
      "    16402    0.692    0.000    0.692    0.000 {method 'read' of 'file' objects}\n",
      "     8192    0.476    0.000    0.651    0.000 <ipython-input-15-d9bd1d33c88e>:1(convert_to_fof_particle)\n",
      "     8210    0.419    0.000    0.419    0.000 {range}\n",
      "       18    0.297    0.016   11.650    0.647 serializers.py:259(dump_stream)\n",
      "     8192    0.133    0.000    0.133    0.000 {numpy.core.multiarray.zeros}\n",
      "     8210    0.087    0.000    0.848    0.000 serializers.py:155(_read_with_length)\n",
      "     8210    0.052    0.000    1.590    0.000 <ipython-input-139-69ff1337d1a3>:20(convert_to_fof_particle_partition)\n",
      "     8210    0.045    0.000    0.092    0.000 serializers.py:542(read_int)\n",
      "     8192    0.042    0.000    0.111    0.000 serializers.py:549(write_int)\n",
      "     8210    0.040    0.000    0.888    0.000 serializers.py:136(load_stream)\n",
      "     8192    0.038    0.000    0.038    0.000 {numpy.core.multiarray.frombuffer}\n",
      "     8192    0.032    0.000    1.754    0.000 serializers.py:414(dumps)\n",
      "     8192    0.023    0.000    0.023    0.000 {_struct.pack}\n",
      "    49152    0.019    0.000    0.019    0.000 {len}\n",
      "     8210    0.017    0.000    0.017    0.000 {_struct.unpack}\n",
      "     8192    0.005    0.000    0.005    0.000 serializers.py:332(loads)\n",
      "        8    0.001    0.000    0.001    0.000 {open}\n",
      "        8    0.000    0.000    0.000    0.000 {cPickle.load}\n",
      "        8    0.000    0.000    0.002    0.000 broadcast.py:82(load)\n",
      "       18    0.000    0.000   11.650    0.647 worker.py:104(process)\n",
      "       18    0.000    0.000    0.002    0.000 broadcast.py:92(value)\n",
      "       18    0.000    0.000    0.000    0.000 rdd.py:2345(pipeline_func)\n",
      "       18    0.000    0.000    0.000    0.000 {hasattr}\n",
      "       18    0.000    0.000    0.000    0.000 {sum}\n",
      "       18    0.000    0.000    0.000    0.000 {iter}\n",
      "       18    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        8    0.000    0.000    0.000    0.000 {gc.enable}\n",
      "        8    0.000    0.000    0.000    0.000 {gc.disable}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc.show_profiles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition particles into domains and set the partition part of local group ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# partitioning duplicates the particles that are located in the boundary regions\n",
    "part_rdd = (p_rdd.mapPartitions(lambda particles: spark_fof.partition_particles_cython(particles, domain_containers, tau, mins, maxs))\n",
    "                 .partitionBy(len(domain_containers))\n",
    "                 .values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the local FOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spark_util import spark_cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import fof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_local_fof(index, particle_iter, tau, nMinMembers): \n",
    "    part_arr = np.fromiter(particle_iter, pdt)\n",
    "    if len(part_arr)>0:\n",
    "        fof.run(part_arr, tau, nMinMembers)\n",
    "    return part_arr\n",
    "\n",
    "def encode_gid(pid, cid, bits=32):\n",
    "    if bits == 32: \n",
    "        res = np.int64(int(np.binary_repr(pid,width=32)+np.binary_repr(cid,width=32),2))\n",
    "    elif bits == 16:\n",
    "        res = np.int32(int(np.binary_repr(pid,width=16)+np.binary_repr(cid,width=16),2))\n",
    "    else: \n",
    "        raise RuntimeError('Group encoding must use either 16 or 32 bit integers')\n",
    "    return res\n",
    "\n",
    "def set_group_id(partition_index, particle_iter):\n",
    "    part_arr = np.fromiter(particle_iter, pdt)\n",
    "    for i in range(len(part_arr)):\n",
    "        #p = np.copy(part_arr[i])\n",
    "        gid = part_arr['iGroup'][i]\n",
    "        part_arr['iGroup'][i] = np.int64(bin((partition_index<<32) | gid), 2)\n",
    "    #    res = int(bin((partition_index<<16) | gid),2)\n",
    "     #   part_arr['iGroup'][i] = res\n",
    "    #    print res, partition_index, gid, spark_fof.encode_gid(partition_index, gid, 16)\n",
    "   # print part_arr[:10]\n",
    "    return part_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fof_rdd = part_rdd.mapPartitionsWithIndex(lambda index, particles: run_local_fof(index, particles, tau, 1))\\\n",
    "                  .mapPartitionsWithIndex(set_group_id).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Merging stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fof_analyzer = spark_fof.FOFAnalyzer(sc, N, tau, fof_rdd, [-.5,-.5,-.5], [.5,.5,.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_rdd = fof_analyzer.merge_groups(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged = merged_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_arr = np.fromiter(merged, pdt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "groups = np.unique(merged_arr['iGroup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7251094"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30065658097"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_arr['iGroup'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "particles = np.fromiter(ps,pdt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ps_0 = np.fromiter(part_rdd.glom().collect()[0], pdt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7251094"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fof.run(particles, tau, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "861190"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fof.run(ps_0, tau, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(1,2, subplot_kw={'aspect':'equal'}, figsize=(12,12))\n",
    "for group in groups: \n",
    "    inds = np.where(merged_arr['iGroup'] == group)[0]\n",
    "    print group, len(inds)\n",
    "    axs[0].plot(merged_arr['pos'][inds,1], merged_arr['pos'][inds,0], ',')\n",
    "    axs[1].plot(merged_arr['pos'][inds,2], merged_arr['pos'][inds,0], ',')\n",
    "for ax in axs:\n",
    "    ax.set_xlim(-1,1); ax.set_ylim(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.show_profiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'particles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-4ebdc17ac1ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time fof.run(particles, tau)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/rokstar/miniconda/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mmagic\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m   2161\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2162\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2163\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2165\u001b[0m     \u001b[0;31m#-------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rokstar/miniconda/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line)\u001b[0m\n\u001b[1;32m   2082\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2083\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2084\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2085\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rokstar/miniconda/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/Users/rokstar/miniconda/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rokstar/miniconda/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1173\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1174\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'particles' is not defined"
     ]
    }
   ],
   "source": [
    "%time fof.run(particles, tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
