#!/bin/sh
#BSUB -J {jobname}
#BSUB -W {walltime} # runtime to request
#BSUB -o {jobname}-%J.log # output extra o means overwrite
#BSUB -n {ncores} # requesting ncores cores
#BSUB -R "span[hosts=-1] rusage[mem={memory}] -R light" # take any available core with mem MB of memory

# setup the spark paths
export SPARK_HOME={spark_home}
export SPARK_LOCAL_DIRS="$__LSF_JOB_TMPDIR__, /scratch/spark-${{USER}}"
export LOCAL_DIRS="$__LSF_JOB_TMPDIR__, /scratch/spark-${{USER}}"
export SPARK_WORKER_DIR="$__LSF_JOB_TMPDIR__/work, /scratch/spark-${{USER}}/work"

sparkcluster launch --memory {memory}M
