{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "def plot_rectangle(rec, ax=None):\n",
    "    if ax is None: \n",
    "        ax = plt.subplot(aspect='equal')\n",
    "    \n",
    "    if isinstance(rec, (list, tuple)):\n",
    "        for r in rec: \n",
    "            plot_rectangle(r,ax)\n",
    "    \n",
    "    else:\n",
    "        size = (rec.maxes-rec.mins)\n",
    "        ax.add_patch(patches.Rectangle(rec.mins, size[0], size[1], fill=False, zorder=-1))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#diff = np.float32(0.033068776)\n",
    "diff = np.float32(0.03306878)\n",
    "global_min = -31*diff\n",
    "global_max = 31*diff\n",
    "\n",
    "dom_maxs = np.array([global_max]*3, dtype=np.float64)\n",
    "dom_mins = np.array([global_min]*3, dtype=np.float64)\n",
    "tau = 0.2/12600 # 0.2 times mean interparticle separation\n",
    "#buffer_tau = diff*5./150.\n",
    "Ngrid = 62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sparkhpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ncores = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SPARK_HOME'] = '/home/ics/roskar/spark'\n",
    "os.environ['SPARK_DRIVER_MEMORY'] = '16G'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sparkhpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sparkhpc.sparkjob:Submitted cluster 0\n"
     ]
    }
   ],
   "source": [
    "clusterid=None\n",
    "\n",
    "if clusterid is None: \n",
    "    sj = sparkhpc.sparkjob.sparkjob(ncores=8,\n",
    "                                memory=50000,\n",
    "                                walltime='08:00', \n",
    "                                cores_per_executor=4, \n",
    "                                memory_per_executor=50000)\n",
    "    sj.wait_to_start()\n",
    "else:\n",
    "    sj = sparkhpc.sparkjob.sparkjob(clusterid=clusterid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<tr><td>ClusterID</td>\n",
       "                    <th>Job ID</th>\n",
       "                    <th>Number of cores</th>\n",
       "                    <th>Status</th>\n",
       "                    <th>Spark UI</th>\n",
       "                    <th>Spark URL</th>\n",
       "                    </tr><tr><td>0</td>\n",
       "                    <td>1169</td>\n",
       "                    <td>8</td>\n",
       "                    <td>running</td>\n",
       "                    <td><a target=\"_blank\" href=\"http://172.19.1.158:8080\">http://172.19.1.158:8080</a></td>\n",
       "                    <td>spark://x09y02:7077</td>\n",
       "                  </tr><tr><td>1</td>\n",
       "                    <td>1425</td>\n",
       "                    <td>8</td>\n",
       "                    <td>running</td>\n",
       "                    <td><a target=\"_blank\" href=\"http://172.19.1.163:8080\">http://172.19.1.163:8080</a></td>\n",
       "                    <td>spark://x09y07:7077</td>\n",
       "                  </tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sj.show_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = sparkhpc.start_spark(master=sj.master_url, spark_conf='../conf', \n",
    "                          profiling=False, executor_memory='16000M', graphframes_package='graphframes:graphframes:0.3.0-spark2.0-s_2.11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.setCheckpointDir('file:///zbox/data/roskar/work/checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_fof: Number of input files:  64\n",
      "spark_fof: Total number of particles:  4695434296\n"
     ]
    }
   ],
   "source": [
    "import spark_fof\n",
    "reload(spark_fof.spark_fof)\n",
    "reload(spark_fof)\n",
    "path = '/zbox/trove/euclid/2Tlc-final/'\n",
    "\n",
    "nMinMembers = 8\n",
    "nBins = 62\n",
    "minblock = 30\n",
    "maxblock = 34\n",
    "fof_analyzer = spark_fof.spark_fof.LCFOFAnalyzer(sc, path, nMinMembers, nBins, tau, dom_mins, dom_maxs, blockids=range(minblock,maxblock), buffer_tau=tau*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "particle_rdd PythonRDD[5] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fof_analyzer.particle_rdd = fof_analyzer.particle_rdd.map(lambda x: np.random.choice(x, size=int(len(x)*.01)), True)\n",
    "fof_analyzer.particle_rdd.setName('particle_rdd')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[13] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fof_analyzer.fof_rdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "group_index_mapping = fof_analyzer.groups_rdd.zipWithIndex().map(lambda ((gid,count), new_gid): (gid, new_gid+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "group_partition_mapping = fof_analyzer.group_mapping.map(lambda (k,v): (v,k>>32)).groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "map_join = group_index_mapping.leftOuterJoin(group_partition_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_map_dict(iterator):\n",
    "    group_map_dict = {g:g_p for g,g_p in iterator}\n",
    "    yield group_map_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten_group_map(args):\n",
    "    g, (g_p, partitions) = args\n",
    "    if partitions is None: \n",
    "        return [(g>>32, (g, g_p)),]\n",
    "    else: \n",
    "        return [(partition, (g, g_p)) for partition in list(partitions) + [g>>32,]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_particle_mapping = (map_join.flatMap(flatten_group_map)\n",
    "                                  .partitionBy(8).map(lambda (k,v): v, True)\n",
    "                                  .mapPartitions(create_map_dict, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remap_group_ids(iterator): \n",
    "    mapping = iterator.next()\n",
    "    \n",
    "    for p_arr in iterator: \n",
    "        spark_fof.spark_fof_c.relabel_groups(p_arr, mapping)\n",
    "        yield p_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_rdd = fof_analyzer.merged_rdd\n",
    "no_ghosts_rdd = merged_rdd.map(lambda p: p[np.where(p['is_ghost'] != spark_fof.spark_fof.GHOST_PARTICLE_COPY)[0]], True)\n",
    "final_part_rdd = (final_particle_mapping + no_ghosts_rdd).mapPartitions(remap_group_ids, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_fof: domain group mapping build took 40.758027 seconds\n",
      "Total number of groups:  16437\n"
     ]
    }
   ],
   "source": [
    "g_counts = fof_analyzer.final_fof_rdd.flatMap(lambda arr: arr['iGroup']).map(lambda g: (g,1)).reduceByKey(lambda a,b: a+b).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_fof: Final group map build took 0.089850 seconds\n"
     ]
    }
   ],
   "source": [
    "groups = fof_analyzer.groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(g_counts[g] == groups[g] for g in g_counts.keys() if g != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def concatenate_partition(iterator): \n",
    "    yield np.concatenate(list(iterator))\n",
    "    \n",
    "def fof_map(part_arr, tau, nMinMembers):\n",
    "    tin = time.time()\n",
    "    t = time.localtime()\n",
    "    print 'spark_fof: starting fof at {time}, pid={pid}'.format(time=time.time(), pid=os.getpid())\n",
    "    # run fof_rdd\n",
    "    tin = time.time()\n",
    "    part_arr.sort(kind='mergesort', order='iOrder')\n",
    "    print 'spark_fof: sorting took %f seconds'%(time.time()-tin)\n",
    "    \n",
    "    spark_fof.spark_fof.fof.run(part_arr, tau, nMinMembers)\n",
    "    print 'spark_fof: finished fof at {time}, pid={pid}'.format(time=time.time(), pid=os.getpid())\n",
    "\n",
    "    # encode the groupID  \n",
    "    #spark_fof_c.encode_gid(part_arr, partition_index)\n",
    "    part_arr['iGroup'] = part_arr['iGroup'] << 32 | part_arr['iGroup'][-1]\n",
    "    return part_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fof = fof_analyzer.partitioned_rdd.mapPartitions(concatenate_partition).map(lambda p: fof_map(p,tau,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32 ms, sys: 5 ms, total: 37 ms\n",
      "Wall time: 5min 14s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time fof.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "merged_rdd PythonRDD[232] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fof_analyzer.merged_rdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_partition(iterator): \n",
    "    i = len(list(iterator))\n",
    "    yield i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fof_analyzer.fof_rdd.mapPartitions(count_partition).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fof_analyzer.fof_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_fof: domain group mapping build took 69.017035 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "merged_rdd PythonRDD[232] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fof_analyzer.merged_rdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_fof: domain group mapping build took 53.798134 seconds\n"
     ]
    }
   ],
   "source": [
    "mapping = fof_analyzer._get_level_map().cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_map_dict(iterator):\n",
    "    group_map_dict = {g:g_p for g,g_p in iterator}\n",
    "    yield group_map_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spark_fof.spark_fof import decode_partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "group_merge_map = (mapping.flatMap(lambda (g,g_p):\n",
    "                            [(gid, (g,g_p)) for gid in [decode_partition(g), decode_partition(g_p)]])\n",
    "                          .partitionBy(8)\n",
    "                          .map(lambda (k,v): v, preservesPartitioning=True)\n",
    "                           .mapPartitions(create_map_dict, True)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fof_analyzer.group_merge_map = group_merge_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_rdd = fof_analyzer.merged_rdd\n",
    "#group_merge_map = fof_analyzer.group_merge_map\n",
    "nPartitions = sc.defaultParallelism*5\n",
    "nMinMembers = fof_analyzer.nMinMembers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pympler import asizeof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_groups_local(i, iterator, nMinMembers):\n",
    "    # the first element is the group mapping dictionary\n",
    "    dist_groups = set(iterator.next().values())\n",
    "    print len(dist_groups)\n",
    "    print 'sizeof set in ', i, ' ', asizeof.asizeof(dist_groups)\n",
    "    group_arrs = np.concatenate([p_arr['iGroup'] for p_arr in iterator])\n",
    "    gids, counts = np.unique(group_arrs, return_counts=True)\n",
    "    return ((g,cnt) for (g,cnt) in zip(gids, counts) if (g in dist_groups) or (cnt >= nMinMembers))\n",
    " \n",
    "def filter_groups_by_map(rdd, not_in_map=False):\n",
    "    def perform_filter(iterator, exclusive):\n",
    "        # the first element after the union is the group mapping\n",
    "        # here we have already remapped the groups so we need to just take the final group IDs\n",
    "        dist_groups = set(iterator.next().values())\n",
    "        return ((gid, count) for (gid,count) in iterator if (gid in dist_groups)^exclusive)\n",
    "\n",
    "    return rdd.mapPartitions(lambda i: perform_filter(i,not_in_map), preservesPartitioning=True)\n",
    "\n",
    "def get_local_groups(rdd, map_rdd): \n",
    "    return filter_groups_by_map(map_rdd + rdd, not_in_map=True)\n",
    "\n",
    "def get_distributed_groups(rdd, map_rdd):\n",
    "    return filter_groups_by_map(map_rdd + rdd, not_in_map=False)\n",
    "\n",
    "# first, get rid of ghost particles\n",
    "no_ghosts_rdd = merged_rdd.map(lambda p: p[np.where(p['is_ghost'] != spark_fof.spark_fof.GHOST_PARTICLE_COPY)[0]], True)\n",
    "\n",
    "# count up the number of particles in each group in each partition\n",
    "group_counts = (group_merge_map + no_ghosts_rdd).mapPartitionsWithIndex(lambda index,i: count_groups_local(index, i, nMinMembers), True).cache()\n",
    "\n",
    "# merge the groups that reside in multiple domains\n",
    "distributed_groups = get_distributed_groups(group_counts, group_merge_map)\n",
    "\n",
    "merge_group_counts = (distributed_groups.reduceByKey(lambda a,b: a+b, nPartitions)\n",
    "                                        .filter(lambda (g,cnt): cnt>=nMinMembers)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_fof: domain group mapping build took 173.421325 seconds\n",
      "Total number of groups:  24568111\n",
      "CPU times: user 740 ms, sys: 260 ms, total: 1e+03 ms\n",
      "Wall time: 1h 37min 6s\n"
     ]
    }
   ],
   "source": [
    "%time fof_analyzer.finalize_groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sparkhpc.sparkjob:\n"
     ]
    }
   ],
   "source": [
    "sj.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
