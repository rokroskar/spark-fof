{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a simple synthetic dataset to test hierarchical merge in FOF algorithm\n",
    "\n",
    "#### the idea is this: \n",
    "\n",
    "* after the local FOF stage, each partition reports the particles it holds in the overlap region\n",
    "* do a reduceByKey or treeAggregate of some sort to collect the groups belonging to the same particles\n",
    "* produce a mapping of $G -> G_1$ and distribute to all hosts in form of broadcast lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.setrecursionlimit(sys.getrecursionlimit()*10)\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SPARK_HOME'] = os.path.join(os.path.expanduser('~'), 'spark')\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_rectangle(rec, ax=None):\n",
    "    if ax is None: \n",
    "        ax = plt.subplot(aspect='equal')\n",
    "    \n",
    "    if isinstance(rec, (list, tuple)):\n",
    "        for r in rec: \n",
    "            plot_rectangle(r,ax)\n",
    "    \n",
    "    else:\n",
    "        size = (rec.maxes-rec.mins)\n",
    "        ax.add_patch(patches.Rectangle(rec.mins, size[0], size[1], fill=False, zorder=-1))\n",
    "    plt.draw()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SPARK_CONF_DIR'] = './conf'\n",
    "os.environ['SPARK_DRIVER_MEMORY'] = '8G'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import pynbody\n",
    "import sparkhpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting job.template\n"
     ]
    }
   ],
   "source": [
    "%%writefile job.template\n",
    "#!/bin/sh\n",
    "#BSUB -J {jobname}\n",
    "#BSUB -W {walltime} # runtime to request\n",
    "#BSUB -o {jobname}-%J.log # output extra o means overwrite\n",
    "#BSUB -n {ncores} # requesting ncores cores\n",
    "#BSUB -R \"rusage[mem={memory}, scratch=10000] span[hosts=-1]\" # take any available core with mem MB of memory\n",
    "\n",
    "# setup the spark paths\n",
    "export SPARK_HOME={spark_home}\n",
    "export SPARK_LOCAL_DIRS=$__LSF_JOB_TMPDIR__\n",
    "export LOCAL_DIRS=$SPARK_LOCAL_DIRS\n",
    "export SPARK_WORKER_DIR=$__LSF_JOB_TMPDIR__/work\n",
    "#export SPARK_CONF_DIR=/cluster/home/roskarr/Projects/spark-fof/conf\n",
    "\n",
    "sparkcluster launch --memory {memory}M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sparkhpc:Job <33425570> is being terminated\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc.stop()\n",
    "sj.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sj = sparkhpc.sparkjob.LSFSparkJob(ncores=27, memory=5500, walltime='4:00', template='./job.template')\n",
    "sj = sparkhpc.sparkjob.LSFSparkJob(ncores=49,memory=12000,walltime='1:00', template='./job.template')\n",
    "#sj = sparkhpc.sparkjob.LSFSparkJob(clusterid=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sparkhpc:Submitted cluster 0\n"
     ]
    }
   ],
   "source": [
    "sj.wait_to_start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = sparkhpc.start_spark(master=sj.master_url, spark_conf='../conf', profiling=True, executor_memory='11500M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<tr>\n",
       "                    <th>Job ID</th>\n",
       "                    <th>Number of cores</th>\n",
       "                    <th>Status</th>\n",
       "                    <th>Spark UI</th>\n",
       "                    <th>Spark URL</th>\n",
       "                    </tr>\n",
       "                    <td>33439584</td>\n",
       "                    <td>49</td>\n",
       "                    <td>running</td>\n",
       "                    <td><a target=\"_blank\" href=\"http://10.205.7.57:8080\">http://10.205.7.57:8080</a></td>\n",
       "                    <td>spark://e1121.hpc-lca.ethz.ch:7077</td>\n",
       "                  "
      ],
      "text/plain": [
       "<sparkhpc.sparkjob.LSFSparkJob at 0x2b235933c790>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run FOF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spark_fof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'spark_fof.spark_fof_c' from '/cluster/project/sis/ri/roskarr/spark-fof/spark_fof/spark_fof_c.so'>"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(spark_fof.spark_fof)\n",
    "reload(spark_fof)\n",
    "reload(spark_fof.spark_fof_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "sj.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input files:  343\n",
      "Total number of particles:  <built-in method values of dict object at 0x2b237419f5c8>\n",
      "CPU times: user 7.92 s, sys: 315 ms, total: 8.24 s\n",
      "Wall time: 8.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#path = '/cluster/home/roskarr/work/2Tlc-final/'\n",
    "path = '/cluster/home/roskarr/projects/euclid/2Tlc-final/'\n",
    "\n",
    "# domain parameters\n",
    "diff = np.float32(0.033068776)\n",
    "global_min = -31*diff\n",
    "global_max = 31*diff\n",
    "\n",
    "dom_maxs = np.array([global_max]*3, dtype=np.float64)\n",
    "dom_mins = np.array([global_min]*3, dtype=np.float64)\n",
    "\n",
    "#tau = diff*5./125.\n",
    "tau = 0.2/12600\n",
    "buffer_tau = diff*5./150.\n",
    "\n",
    "fof_analyzer = spark_fof.spark_fof.LCFOFAnalyzer(sc, path, 64, 62, tau, dom_mins, dom_maxs, blockids=[30,31,32,33,34,35,36,37], buffer_tau=tau*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job 0 cancelled because Stage 2 was cancelled\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1393)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply$mcVI$sp(DAGScheduler.scala:1381)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply(DAGScheduler.scala:1380)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply(DAGScheduler.scala:1380)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofInt.foreach(ArrayOps.scala:234)\n\tat org.apache.spark.scheduler.DAGScheduler.handleStageCancellation(DAGScheduler.scala:1380)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1636)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1890)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1903)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1916)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor109.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:483)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-169-a8030c2b59a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu'len(fof_analyzer.groups)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/cluster/home/roskarr/miniconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2113\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/cluster/home/roskarr/miniconda2/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/home/roskarr/miniconda2/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/cluster/project/sis/ri/roskarr/spark-fof/spark_fof/spark_fof.py\u001b[0m in \u001b[0;36mgroups\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_groups\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_fof_rdd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_groups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/project/sis/ri/roskarr/spark-fof/spark_fof/spark_fof.py\u001b[0m in \u001b[0;36mfinal_fof_rdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfinal_fof_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_fof_rdd\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_fof_rdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalize_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_fof_rdd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/project/sis/ri/roskarr/spark-fof/spark_fof/spark_fof.py\u001b[0m in \u001b[0;36mfinalize_groups\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mrelabel_groups_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0mrelabel_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mp_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0mmerged_rdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerged_rdd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/project/sis/ri/roskarr/spark-fof/spark_fof/spark_fof.py\u001b[0m in \u001b[0;36mmerged_rdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmerged_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merged_rdd\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merged_rdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merged_rdd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/project/sis/ri/roskarr/spark-fof/spark_fof/spark_fof.py\u001b[0m in \u001b[0;36m_merge_groups\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mp_arr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparticles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0mremap_gid_partition_cython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mp_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_level_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/project/sis/ri/roskarr/spark-fof/spark_fof/spark_fof.py\u001b[0m in \u001b[0;36m_get_level_map\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;31m# create the spark GraphFrame with group IDs as nodes and group connections as edges\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mv_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroups_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistinct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0me_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroups_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mg_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraphframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/home/roskarr/spark/python/pyspark/sql/context.pyc\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mPy4JJavaError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \"\"\"\n\u001b[0;32m--> 309\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/home/roskarr/spark/python/pyspark/sql/session.pyc\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/home/roskarr/spark/python/pyspark/sql/session.pyc\u001b[0m in \u001b[0;36m_createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    358\u001b[0m         \"\"\"\n\u001b[1;32m    359\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/home/roskarr/spark/python/pyspark/sql/session.pyc\u001b[0m in \u001b[0;36m_inferSchema\u001b[0;34m(self, rdd, samplingRatio)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStructType\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \"\"\"\n\u001b[0;32m--> 331\u001b[0;31m         \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             raise ValueError(\"The first row in RDD is empty, \"\n",
      "\u001b[0;32m/cluster/home/roskarr/spark/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m         \"\"\"\n\u001b[0;32m-> 1328\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1329\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/home/roskarr/spark/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1310\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/home/roskarr/spark/python/pyspark/context.pyc\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m         \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/home/roskarr/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/home/roskarr/spark/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/home/roskarr/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job 0 cancelled because Stage 2 was cancelled\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1393)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply$mcVI$sp(DAGScheduler.scala:1381)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply(DAGScheduler.scala:1380)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply(DAGScheduler.scala:1380)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofInt.foreach(ArrayOps.scala:234)\n\tat org.apache.spark.scheduler.DAGScheduler.handleStageCancellation(DAGScheduler.scala:1380)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1636)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1890)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1903)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1916)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor109.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:483)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "len(fof_analyzer.groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.55 s, sys: 206 ms, total: 3.76 s\n",
      "Wall time: 9min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from itertools import izip \n",
    "\n",
    "nMinMembers = 64\n",
    "\n",
    "def count_groups_partition(particle_arrays, gr_map_inv_b, nMinMembers): \n",
    "    p_arr = np.concatenate(list(particle_arrays))\n",
    "    gs, counts = np.unique(p_arr['iGroup'], return_counts=True)\n",
    "    gr_map_inv = gr_map_inv_b.value\n",
    "    return ((g,cnt) for g,cnt in izip(gs,counts) if (g in gr_map_inv) or (cnt >= nMinMembers))\n",
    "\n",
    "merged_rdd = fof_analyzer.merged_rdd\n",
    "\n",
    "group_merge_map = fof_analyzer.group_merge_map\n",
    "gr_map_inv = {v:k for (k,v) in group_merge_map.iteritems()}\n",
    "gr_map_inv_b = sc.broadcast(gr_map_inv)\n",
    "\n",
    "# first, get rid of ghost particles\n",
    "no_ghosts_rdd = merged_rdd.map(lambda p: p[np.where(p['is_ghost'] != spark_fof.spark_fof.GHOST_PARTICLE_COPY)[0]])\n",
    "\n",
    "# count up the number of particles in each group in each partition\n",
    "group_counts = no_ghosts_rdd.mapPartitions(lambda p_arrs: count_groups_partition(p_arrs, gr_map_inv_b, nMinMembers)).cache()\n",
    "group_counts.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge the groups that reside in multiple domains\n",
    "merge_group_counts = (group_counts.filter(lambda (g,cnt): g in gr_map_inv_b.value)\n",
    "                                  .reduceByKey(lambda a,b: a+b)\n",
    "                                  .filter(lambda (g,cnt): cnt>=nMinMembers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combine the group counts\n",
    "total_group_counts = (group_counts.filter(lambda (gid,cnt): gid not in gr_map_inv_b.value) + merge_group_counts).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[460] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fof_analyzer.fof_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqc = sparkhpc.get_sqc(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype({'names':['pos','is_ghost','iOrder','iGroup'], 'formats':[('<f4', (3,)),'<i4','<i4','<i8'], 'offsets':[0,12,16,24], 'itemsize':32}, align=True)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_fof.spark_fof.pdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pid_gid_ghost(p):\n",
    "    return Row(gid=int(p['iGroup']),pid=int(p['iOrder']),is_ghost=int(p['is_ghost']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fof_gpghost = sqc.createDataFrame(fof_analyzer.fof_rdd.flatMap(lambda p: p).map(pid_gid_ghost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fof_rdd = fof_analyzer.fof_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = fof_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fof_gpghost = sqc.createDataFrame(fof_rdd.flatMap(lambda arr: list(arr[['iGroup','iOrder','is_ghost']]))\n",
    "                                               .map(pid_gid_ghost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+\n",
      "|gid|is_ghost|pid|\n",
      "+---+--------+---+\n",
      "| 12|       1|  0|\n",
      "| 12|       1|  1|\n",
      "| 12|       1|  2|\n",
      "| 12|       1|  3|\n",
      "| 12|       1|  4|\n",
      "| 12|       1|  5|\n",
      "| 25|       1|  6|\n",
      "| 25|       1|  7|\n",
      "| 25|       1|  8|\n",
      "| 25|       0|  9|\n",
      "| 25|       0| 10|\n",
      "| 25|       0| 11|\n",
      "| 25|       0| 12|\n",
      "| 25|       0| 13|\n",
      "| 25|       0| 14|\n",
      "| 25|       0| 15|\n",
      "| 25|       0| 16|\n",
      "| 25|       0| 17|\n",
      "|241|       0| 18|\n",
      "|241|       0| 19|\n",
      "+---+--------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fof_gpghost.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "groups = fof_gpghost.groupBy('pid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linked_groups = groups.count().filter('count > 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1981.count.\n: org.apache.spark.SparkException: Job 70 cancelled because Stage 728 was cancelled\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1393)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply$mcVI$sp(DAGScheduler.scala:1381)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply(DAGScheduler.scala:1380)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply(DAGScheduler.scala:1380)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofInt.foreach(ArrayOps.scala:234)\n\tat org.apache.spark.scheduler.DAGScheduler.handleStageCancellation(DAGScheduler.scala:1380)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1636)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1890)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1903)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1916)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1930)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:911)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:290)\n\tat org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2193)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2546)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2192)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2199)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2227)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2226)\n\tat org.apache.spark.sql.Dataset.withCallback(Dataset.scala:2559)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2226)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:483)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-123-e5dd5d7f2c2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu'linked_groups.count()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/cluster/home/roskarr/miniconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2113\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/cluster/home/roskarr/miniconda2/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/home/roskarr/miniconda2/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/cluster/home/roskarr/spark/python/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \"\"\"\n\u001b[0;32m--> 299\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/home/roskarr/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/home/roskarr/spark/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/home/roskarr/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1981.count.\n: org.apache.spark.SparkException: Job 70 cancelled because Stage 728 was cancelled\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1393)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply$mcVI$sp(DAGScheduler.scala:1381)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply(DAGScheduler.scala:1380)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply(DAGScheduler.scala:1380)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofInt.foreach(ArrayOps.scala:234)\n\tat org.apache.spark.scheduler.DAGScheduler.handleStageCancellation(DAGScheduler.scala:1380)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1636)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1890)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1903)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1916)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1930)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:911)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:290)\n\tat org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2193)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2546)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2192)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2199)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2227)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2226)\n\tat org.apache.spark.sql.Dataset.withCallback(Dataset.scala:2559)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2226)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:483)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "linked_groups.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.592592592592593"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7550*.02/27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1493.count.\n: org.apache.spark.SparkException: Job 40 cancelled because Stage 636 was cancelled\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1393)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply$mcVI$sp(DAGScheduler.scala:1381)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply(DAGScheduler.scala:1380)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply(DAGScheduler.scala:1380)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofInt.foreach(ArrayOps.scala:234)\n\tat org.apache.spark.scheduler.DAGScheduler.handleStageCancellation(DAGScheduler.scala:1380)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1636)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1890)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1903)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1916)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1930)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:911)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:290)\n\tat org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2193)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2546)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2192)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2199)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2227)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2226)\n\tat org.apache.spark.sql.Dataset.withCallback(Dataset.scala:2559)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2226)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:483)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-c829db4f92ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfof_gpghost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/cluster/home/roskarr/spark/python/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \"\"\"\n\u001b[0;32m--> 299\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/home/roskarr/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/home/roskarr/spark/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/home/roskarr/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1493.count.\n: org.apache.spark.SparkException: Job 40 cancelled because Stage 636 was cancelled\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1393)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply$mcVI$sp(DAGScheduler.scala:1381)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply(DAGScheduler.scala:1380)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply(DAGScheduler.scala:1380)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofInt.foreach(ArrayOps.scala:234)\n\tat org.apache.spark.scheduler.DAGScheduler.handleStageCancellation(DAGScheduler.scala:1380)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1636)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1890)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1903)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1916)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1930)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:911)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:290)\n\tat org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2193)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2546)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2192)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2199)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2227)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2226)\n\tat org.apache.spark.sql.Dataset.withCallback(Dataset.scala:2559)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2226)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:483)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "fof_gpghost.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ghost_counts(index, iterator): \n",
    "    nghosts = 0\n",
    "    nghosts_copy = 0\n",
    "    nother = 0\n",
    "    for arr in iterator: \n",
    "        nghosts += len(np.where(arr['is_ghost']==1)[0])\n",
    "        nghosts_copy += len(np.where(arr['is_ghost']==2)[0])\n",
    "        nother += len(np.where(arr['is_ghost']==0)[0])\n",
    "    yield index,nother,nghosts,nghosts_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fof_rdd = fof_analyzer.fof_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15456"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fof_rdd.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 5978.56 MiB, increment: 5271.09 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit groups = fof_analyzer._get_level_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cPickle.dump(groups, open('groups_map.pickle','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "final_rdd = fof_analyzer.final_fof_rdd\n",
    "final_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2876775"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1267851"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fof_analyzer2.groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sparkhpc:Job <33251605> is being terminated\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc.stop()\n",
    "sj.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Profile of RDD<id=171>\n",
      "============================================================\n",
      "         1593 function calls in 0.014 seconds\n",
      "\n",
      "   Ordered by: internal time, cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "       81    0.002    0.000    0.011    0.000 rdd.py:1713(add_shuffle_key)\n",
      "       27    0.002    0.000    0.005    0.000 broadcast.py:82(load)\n",
      "       27    0.002    0.000    0.002    0.000 {cPickle.load}\n",
      "       27    0.001    0.000    0.001    0.000 {open}\n",
      "       27    0.001    0.000    0.001    0.000 {cPickle.dumps}\n",
      "       54    0.001    0.000    0.001    0.000 serializers.py:143(_write_with_length)\n",
      "       27    0.001    0.000    0.001    0.000 serializers.py:217(load_stream)\n",
      "       27    0.000    0.000    0.001    0.000 rdd.py:61(portable_hash)\n",
      "       27    0.000    0.000    0.006    0.000 broadcast.py:92(value)\n",
      "       27    0.000    0.000    0.014    0.001 worker.py:165(process)\n",
      "       54    0.000    0.000    0.001    0.000 serializers.py:155(_read_with_length)\n",
      "       27    0.000    0.000    0.013    0.000 serializers.py:132(dump_stream)\n",
      "       27    0.000    0.000    0.006    0.000 spark_fof.py:590(<lambda>)\n",
      "       54    0.000    0.000    0.000    0.000 serializers.py:542(read_int)\n",
      "       27    0.000    0.000    0.001    0.000 serializers.py:414(dumps)\n",
      "       54    0.000    0.000    0.000    0.000 serializers.py:549(write_int)\n",
      "       54    0.000    0.000    0.001    0.000 serializers.py:136(load_stream)\n",
      "       27    0.000    0.000    0.000    0.000 {cPickle.loads}\n",
      "      108    0.000    0.000    0.000    0.000 {method 'write' of 'file' objects}\n",
      "       27    0.000    0.000    0.000    0.000 rdd.py:2370(pipeline_func)\n",
      "       27    0.000    0.000    0.000    0.000 serializers.py:538(pack_long)\n",
      "       27    0.000    0.000    0.000    0.000 {hasattr}\n",
      "       27    0.000    0.000    0.000    0.000 {isinstance}\n",
      "       81    0.000    0.000    0.000    0.000 {_struct.pack}\n",
      "       27    0.000    0.000    0.000    0.000 serializers.py:421(loads)\n",
      "       81    0.000    0.000    0.000    0.000 {method 'read' of 'file' objects}\n",
      "       27    0.000    0.000    0.000    0.000 {gc.disable}\n",
      "       27    0.000    0.000    0.000    0.000 serializers.py:220(_load_stream_without_unbatching)\n",
      "       27    0.000    0.000    0.000    0.000 {built-in method from_iterable}\n",
      "       27    0.000    0.000    0.000    0.000 rdd.py:288(func)\n",
      "       27    0.000    0.000    0.000    0.000 {min}\n",
      "       54    0.000    0.000    0.000    0.000 {_struct.unpack}\n",
      "      135    0.000    0.000    0.000    0.000 {len}\n",
      "       27    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}\n",
      "       27    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
      "       54    0.000    0.000    0.000    0.000 serializers.py:335(dumps)\n",
      "       27    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "       27    0.000    0.000    0.000    0.000 {hash}\n",
      "       27    0.000    0.000    0.000    0.000 {gc.enable}\n",
      "\n",
      "\n",
      "============================================================\n",
      "Profile of RDD<id=175>\n",
      "============================================================\n",
      "         213408 function calls (213354 primitive calls) in 763.400 seconds\n",
      "\n",
      "   Ordered by: internal time, cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "    19413  701.221    0.036  701.221    0.036 {method 'read' of 'file' objects}\n",
      "    19305   42.210    0.002  763.255    0.040 spark_fof.py:547(read_file)\n",
      "    19278   19.012    0.001   19.012    0.001 {numpy.core.multiarray.zeros}\n",
      "    19278    0.250    0.000    0.250    0.000 {numpy.core.multiarray.frombuffer}\n",
      "    19278    0.179    0.000    0.212    0.000 spark_fof.py:428(addInPlace)\n",
      "       27    0.150    0.006    0.150    0.006 {open}\n",
      "    19278    0.131    0.000    0.343    0.000 accumulators.py:160(add)\n",
      "    19305    0.102    0.000  763.357    0.040 rdd.py:1008(<genexpr>)\n",
      "    57942    0.066    0.000    0.066    0.000 {len}\n",
      "       54    0.040    0.001  763.397   14.137 {sum}\n",
      "    19278    0.033    0.000    0.033    0.000 {method 'iteritems' of 'dict' objects}\n",
      "       27    0.001    0.000    0.002    0.000 serializers.py:259(dump_stream)\n",
      "       54    0.001    0.000    0.002    0.000 serializers.py:155(_read_with_length)\n",
      "       27    0.000    0.000  763.400   28.274 worker.py:165(process)\n",
      "       54    0.000    0.000    0.002    0.000 serializers.py:136(load_stream)\n",
      "       54    0.000    0.000    0.001    0.000 serializers.py:542(read_int)\n",
      "       27    0.000    0.000    0.000    0.000 {cPickle.dumps}\n",
      "       27    0.000    0.000    0.000    0.000 {cPickle.loads}\n",
      "    81/27    0.000    0.000  763.398   28.274 rdd.py:2370(pipeline_func)\n",
      "       54    0.000    0.000    0.000    0.000 {method 'write' of 'file' objects}\n",
      "       54    0.000    0.000    0.000    0.000 {_struct.unpack}\n",
      "       81    0.000    0.000  763.397    9.425 rdd.py:316(func)\n",
      "       27    0.000    0.000    0.000    0.000 serializers.py:217(load_stream)\n",
      "       27    0.000    0.000  763.397   28.274 rdd.py:1008(<lambda>)\n",
      "       27    0.000    0.000    0.000    0.000 serializers.py:549(write_int)\n",
      "       27    0.000    0.000    0.000    0.000 {operator.add}\n",
      "       27    0.000    0.000    0.000    0.000 serializers.py:414(dumps)\n",
      "       27    0.000    0.000    0.000    0.000 {_struct.pack}\n",
      "       54    0.000    0.000    0.000    0.000 rdd.py:865(func)\n",
      "       27    0.000    0.000    0.000    0.000 serializers.py:421(loads)\n",
      "       27    0.000    0.000    0.000    0.000 serializers.py:220(_load_stream_without_unbatching)\n",
      "       27    0.000    0.000    0.000    0.000 rdd.py:999(<lambda>)\n",
      "       54    0.000    0.000    0.000    0.000 {time.time}\n",
      "       27    0.000    0.000    0.000    0.000 {built-in method from_iterable}\n",
      "       27    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "       27    0.000    0.000    0.000    0.000 {iter}\n",
      "\n",
      "\n",
      "============================================================\n",
      "Profile of RDD<id=176>\n",
      "============================================================\n",
      "         4853634 function calls (4814943 primitive calls) in 520.473 seconds\n",
      "\n",
      "   Ordered by: internal time, cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "    19278  137.343    0.007  137.343    0.007 spark_fof_c.pyx:223(ghost_mask)\n",
      "    19305  137.022    0.007  244.886    0.013 spark_fof.py:533(set_particle_IDs_partition)\n",
      "    19278  118.036    0.006  134.196    0.007 spark_fof_c.pyx:133(partition_ghosts)\n",
      "    19305   41.645    0.002   80.316    0.004 spark_fof.py:547(read_file)\n",
      "    19305   27.512    0.001   27.512    0.001 {range}\n",
      "    19278   19.962    0.001   19.962    0.001 {numpy.core.multiarray.zeros}\n",
      "    19413   18.248    0.001   18.248    0.001 {method 'read' of 'file' objects}\n",
      "    19278    5.700    0.000    5.700    0.000 {method 'nonzero' of 'numpy.ndarray' objects}\n",
      "77112/38556    4.305    0.000    7.164    0.000 _internal.py:423(_dtype_from_pep3118)\n",
      "    57834    1.136    0.000    8.309    0.000 {numpy.core.multiarray.array}\n",
      "   289170    1.083    0.000    1.083    0.000 stringsource:341(__cinit__)\n",
      "      656    0.765    0.001    0.765    0.001 {method 'write' of 'file' objects}\n",
      "    38556    0.720    0.000    1.018    0.000 _internal.py:22(_makenames_list)\n",
      "    38556    0.574    0.000    1.592    0.000 _internal.py:55(_usefields)\n",
      "    19278    0.548    0.000    0.930    0.000 arraysetops.py:96(unique)\n",
      "   192780    0.401    0.000    0.401    0.000 _internal.py:436(get_dummy_name)\n",
      "      164    0.351    0.002    0.351    0.002 {cPickle.dumps}\n",
      "    19278    0.343    0.000  138.654    0.007 {spark_fof.spark_fof_c.ghost_mask}\n",
      "    17691    0.330    0.000  518.904    0.029 spark_fof.py:170(partition_helper)\n",
      "   250614    0.290    0.000    1.357    0.000 stringsource:643(memoryview_cwrapper)\n",
      "    38556    0.263    0.000    0.284    0.000 stringsource:985(memoryview_fromslice)\n",
      "   347004    0.215    0.000    0.215    0.000 _internal.py:624(_gcd)\n",
      "    38556    0.212    0.000    0.212    0.000 {map}\n",
      "    19278    0.204    0.000  134.840    0.007 {spark_fof.spark_fof_c.partition_ghosts}\n",
      "    19305    0.193    0.000  383.733    0.020 spark_fof.py:146(ghost_map_wrapper)\n",
      "    15257    0.178    0.000    0.178    0.000 {numpy.core.multiarray.concatenate}\n",
      "    38556    0.177    0.000    0.208    0.000 {method 'sort' of 'list' objects}\n",
      "      355    0.174    0.000  519.692    1.464 rdd.py:1713(add_shuffle_key)\n",
      "    19278    0.163    0.000  137.551    0.007 spark_fof_c.pyx:223(ghost_mask (wrapper))\n",
      "    38556    0.163    0.000    8.457    0.000 numeric.py:414(asarray)\n",
      "    19278    0.161    0.000    8.916    0.000 fromnumeric.py:1490(nonzero)\n",
      "    19278    0.160    0.000    8.754    0.000 fromnumeric.py:43(_wrapit)\n",
      "    19278    0.156    0.000    0.184    0.000 spark_fof.py:428(addInPlace)\n",
      "    19278    0.152    0.000    0.188    0.000 _internal.py:277(_newnames)\n",
      "    19278    0.134    0.000    0.318    0.000 accumulators.py:160(add)\n",
      "   308448    0.129    0.000    0.129    0.000 {method 'startswith' of 'str' objects}\n",
      "    17664    0.117    0.000    0.166    0.000 spark_fof.py:199(<lambda>)\n",
      "   192780    0.113    0.000    0.113    0.000 {method 'index' of 'str' objects}\n",
      "    19278    0.108    0.000    0.108    0.000 {method 'flatten' of 'numpy.ndarray' objects}\n",
      "   328545    0.104    0.000    0.104    0.000 {len}\n",
      "    19278    0.092    0.000    0.092    0.000 {numpy.core.multiarray.frombuffer}\n",
      "   289170    0.085    0.000    0.085    0.000 stringsource:368(__dealloc__)\n",
      "   308448    0.063    0.000    0.063    0.000 {method 'get' of 'dict' objects}\n",
      "   204478    0.055    0.000    0.055    0.000 {isinstance}\n",
      "   347004    0.050    0.000    0.050    0.000 {method 'isdigit' of 'str' objects}\n",
      "   250614    0.050    0.000    0.050    0.000 stringsource:649(memoryview_check)\n",
      "    15257    0.048    0.000    0.048    0.000 {method 'sort' of 'numpy.ndarray' objects}\n",
      "    29389    0.041    0.000    0.060    0.000 broadcast.py:92(value)\n",
      "    11698    0.041    0.000    0.065    0.000 rdd.py:61(portable_hash)\n",
      "    38556    0.034    0.000    0.034    0.000 {method 'split' of 'str' objects}\n",
      "    19278    0.034    0.000    0.048    0.000 numeric.py:484(asanyarray)\n",
      "   165922    0.032    0.000    0.032    0.000 {method 'append' of 'list' objects}\n",
      "   154224    0.032    0.000    0.032    0.000 _internal.py:44(<lambda>)\n",
      "    38556    0.030    0.000    0.030    0.000 _internal.py:433(next_dummy_name)\n",
      "    19278    0.028    0.000    0.028    0.000 {method 'iteritems' of 'dict' objects}\n",
      "    19278    0.027    0.000    0.027    0.000 {method 'remove' of 'list' objects}\n",
      "    19278    0.027    0.000    0.027    0.000 {getattr}\n",
      "       81    0.020    0.000    0.020    0.000 {open}\n",
      "    11698    0.018    0.000    0.026    0.000 spark_fof.py:200(<lambda>)\n",
      "    38575    0.018    0.000    0.018    0.000 {method 'keys' of 'dict' objects}\n",
      "       27    0.012    0.000  520.472   19.277 serializers.py:132(dump_stream)\n",
      "    29389    0.012    0.000    0.012    0.000 {hasattr}\n",
      "    38556    0.008    0.000    0.008    0.000 stringsource:507(__getbuffer__)\n",
      "    38556    0.007    0.000    0.007    0.000 stringsource:962(__dealloc__)\n",
      "    38556    0.006    0.000    0.006    0.000 stringsource:545(__get__)\n",
      "       54    0.002    0.000    0.002    0.000 {cPickle.load}\n",
      "    11698    0.002    0.000    0.002    0.000 {hash}\n",
      "      328    0.002    0.000    0.768    0.002 serializers.py:143(_write_with_length)\n",
      "       54    0.001    0.000    0.007    0.000 broadcast.py:82(load)\n",
      "      492    0.001    0.000    0.001    0.000 {_struct.pack}\n",
      "      328    0.001    0.000    0.003    0.000 serializers.py:549(write_int)\n",
      "      164    0.000    0.000    0.001    0.000 serializers.py:538(pack_long)\n",
      "      164    0.000    0.000    0.352    0.002 serializers.py:414(dumps)\n",
      "   162/27    0.000    0.000    0.001    0.000 rdd.py:2370(pipeline_func)\n",
      "       54    0.000    0.000    0.001    0.000 serializers.py:155(_read_with_length)\n",
      "       54    0.000    0.000    0.002    0.000 serializers.py:136(load_stream)\n",
      "       27    0.000    0.000  520.473   19.277 worker.py:165(process)\n",
      "       54    0.000    0.000    0.001    0.000 serializers.py:542(read_int)\n",
      "       27    0.000    0.000    0.000    0.000 serializers.py:217(load_stream)\n",
      "       27    0.000    0.000    0.000    0.000 {cPickle.loads}\n",
      "       54    0.000    0.000    0.000    0.000 {_struct.unpack}\n",
      "       81    0.000    0.000    0.000    0.000 rdd.py:316(func)\n",
      "      328    0.000    0.000    0.000    0.000 serializers.py:335(dumps)\n",
      "       27    0.000    0.000    0.000    0.000 serializers.py:220(_load_stream_without_unbatching)\n",
      "       27    0.000    0.000    0.000    0.000 {min}\n",
      "       27    0.000    0.000    0.000    0.000 rdd.py:366(func)\n",
      "       27    0.000    0.000    0.000    0.000 serializers.py:421(loads)\n",
      "       27    0.000    0.000    0.000    0.000 {sum}\n",
      "       54    0.000    0.000    0.000    0.000 {time.time}\n",
      "       27    0.000    0.000    0.000    0.000 {built-in method from_iterable}\n",
      "       27    0.000    0.000    0.000    0.000 rdd.py:288(func)\n",
      "       27    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
      "       54    0.000    0.000    0.000    0.000 {gc.disable}\n",
      "       27    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "       54    0.000    0.000    0.000    0.000 {gc.enable}\n",
      "\n",
      "\n",
      "============================================================\n",
      "Profile of RDD<id=180>\n",
      "============================================================\n",
      "         134072 function calls in 1.209 seconds\n",
      "\n",
      "   Ordered by: internal time, cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "      429    0.324    0.001    0.324    0.001 {cPickle.dumps}\n",
      "      858    0.307    0.000    0.307    0.000 {method 'write' of 'file' objects}\n",
      "    11698    0.179    0.000    0.179    0.000 spark_fof_c.pyx:84(remap_gid_partition_cython)\n",
      "      164    0.160    0.001    0.160    0.001 {cPickle.loads}\n",
      "      355    0.089    0.000    0.089    0.000 {method 'read' of 'file' objects}\n",
      "    11698    0.039    0.000    0.039    0.000 stringsource:341(__cinit__)\n",
      "       27    0.037    0.001    1.207    0.045 serializers.py:259(dump_stream)\n",
      "    11698    0.016    0.000    0.250    0.000 {spark_fof.spark_fof_c.remap_gid_partition_cython}\n",
      "    11698    0.013    0.000    0.274    0.000 spark_fof.py:193(remap_partition)\n",
      "    11698    0.007    0.000    0.045    0.000 stringsource:643(memoryview_cwrapper)\n",
      "    11698    0.007    0.000    0.281    0.000 spark_fof.py:202(<lambda>)\n",
      "    11698    0.006    0.000    0.187    0.000 spark_fof_c.pyx:84(remap_gid_partition_cython (wrapper))\n",
      "    11698    0.006    0.000    0.011    0.000 broadcast.py:92(value)\n",
      "      191    0.003    0.000    0.254    0.001 serializers.py:136(load_stream)\n",
      "    11698    0.003    0.000    0.003    0.000 {hasattr}\n",
      "       27    0.002    0.000    1.209    0.045 worker.py:165(process)\n",
      "      191    0.002    0.000    0.252    0.001 serializers.py:155(_read_with_length)\n",
      "    11698    0.001    0.000    0.001    0.000 stringsource:368(__dealloc__)\n",
      "    11698    0.001    0.000    0.001    0.000 stringsource:649(memoryview_check)\n",
      "      429    0.001    0.000    0.004    0.000 serializers.py:549(write_int)\n",
      "      429    0.001    0.000    0.001    0.000 {_struct.pack}\n",
      "       27    0.001    0.000    0.001    0.000 {open}\n",
      "      429    0.001    0.000    0.325    0.001 serializers.py:414(dumps)\n",
      "       27    0.001    0.000    0.003    0.000 broadcast.py:82(load)\n",
      "      191    0.001    0.000    0.001    0.000 serializers.py:542(read_int)\n",
      "       27    0.001    0.000    0.001    0.000 {cPickle.load}\n",
      "     1022    0.000    0.000    0.000    0.000 {len}\n",
      "      164    0.000    0.000    0.160    0.001 serializers.py:421(loads)\n",
      "      191    0.000    0.000    0.000    0.000 {_struct.unpack}\n",
      "       27    0.000    0.000    0.000    0.000 serializers.py:217(load_stream)\n",
      "       27    0.000    0.000    0.000    0.000 serializers.py:220(_load_stream_without_unbatching)\n",
      "       27    0.000    0.000    0.000    0.000 {built-in method from_iterable}\n",
      "       27    0.000    0.000    0.000    0.000 rdd.py:288(func)\n",
      "       27    0.000    0.000    0.000    0.000 {iter}\n",
      "       27    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "       27    0.000    0.000    0.000    0.000 {gc.enable}\n",
      "       27    0.000    0.000    0.000    0.000 {gc.disable}\n",
      "\n",
      "\n",
      "============================================================\n",
      "Profile of RDD<id=181>\n",
      "============================================================\n",
      "         1042146 function calls (1042119 primitive calls) in 754.842 seconds\n",
      "\n",
      "   Ordered by: internal time, cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "    38556  227.423    0.006  227.423    0.006 {method 'write' of 'file' objects}\n",
      "    19305  141.489    0.007  306.584    0.016 spark_fof.py:533(set_particle_IDs_partition)\n",
      "    19278  137.560    0.007  137.560    0.007 spark_fof_c.pyx:223(ghost_mask)\n",
      "    19278   79.661    0.004   79.661    0.004 {cPickle.dumps}\n",
      "    19413   77.698    0.004   77.698    0.004 {method 'read' of 'file' objects}\n",
      "    19305   41.730    0.002  138.392    0.007 spark_fof.py:547(read_file)\n",
      "    19305   26.664    0.001   26.664    0.001 {range}\n",
      "    19278   18.278    0.001   18.278    0.001 {numpy.core.multiarray.zeros}\n",
      "       27    1.522    0.056  754.842   27.957 serializers.py:259(dump_stream)\n",
      "   134946    0.622    0.000    0.622    0.000 stringsource:341(__cinit__)\n",
      "    19278    0.364    0.000  138.980    0.007 {spark_fof.spark_fof_c.ghost_mask}\n",
      "    19305    0.228    0.000  445.792    0.023 spark_fof.py:146(ghost_map_wrapper)\n",
      "    19278    0.219    0.000    0.219    0.000 {numpy.core.multiarray.frombuffer}\n",
      "    19278    0.186    0.000    0.218    0.000 spark_fof.py:428(addInPlace)\n",
      "   134946    0.180    0.000    0.802    0.000 stringsource:643(memoryview_cwrapper)\n",
      "    19278    0.177    0.000  137.786    0.007 spark_fof_c.pyx:223(ghost_mask (wrapper))\n",
      "    19278    0.155    0.000    0.373    0.000 accumulators.py:160(add)\n",
      "   135000    0.151    0.000    0.151    0.000 {len}\n",
      "    19278    0.130    0.000    0.130    0.000 {_struct.pack}\n",
      "    19278    0.122    0.000    0.455    0.000 serializers.py:549(write_int)\n",
      "    19278    0.121    0.000   79.782    0.004 serializers.py:414(dumps)\n",
      "       54    0.050    0.001    0.050    0.001 {open}\n",
      "   134946    0.050    0.000    0.050    0.000 stringsource:368(__dealloc__)\n",
      "    19278    0.031    0.000    0.031    0.000 {method 'iteritems' of 'dict' objects}\n",
      "   134946    0.029    0.000    0.029    0.000 stringsource:649(memoryview_check)\n",
      "       27    0.001    0.000    0.001    0.000 {cPickle.load}\n",
      "       27    0.001    0.000    0.003    0.000 broadcast.py:82(load)\n",
      "       54    0.000    0.000    0.001    0.000 serializers.py:155(_read_with_length)\n",
      "       54    0.000    0.000    0.002    0.000 serializers.py:136(load_stream)\n",
      "       54    0.000    0.000    0.001    0.000 serializers.py:542(read_int)\n",
      "       27    0.000    0.000  754.842   27.957 worker.py:165(process)\n",
      "       27    0.000    0.000    0.000    0.000 serializers.py:217(load_stream)\n",
      "       27    0.000    0.000    0.000    0.000 {cPickle.loads}\n",
      "       54    0.000    0.000    0.000    0.000 {_struct.unpack}\n",
      "       27    0.000    0.000    0.003    0.000 broadcast.py:92(value)\n",
      "    54/27    0.000    0.000    0.000    0.000 rdd.py:2370(pipeline_func)\n",
      "       27    0.000    0.000    0.000    0.000 serializers.py:220(_load_stream_without_unbatching)\n",
      "       27    0.000    0.000    0.000    0.000 serializers.py:421(loads)\n",
      "       54    0.000    0.000    0.000    0.000 {time.time}\n",
      "       27    0.000    0.000    0.000    0.000 {sum}\n",
      "       27    0.000    0.000    0.000    0.000 {hasattr}\n",
      "       27    0.000    0.000    0.000    0.000 {built-in method from_iterable}\n",
      "       27    0.000    0.000    0.000    0.000 rdd.py:316(func)\n",
      "       27    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "       27    0.000    0.000    0.000    0.000 {iter}\n",
      "       27    0.000    0.000    0.000    0.000 {gc.disable}\n",
      "       27    0.000    0.000    0.000    0.000 {gc.enable}\n",
      "\n",
      "\n",
      "============================================================\n",
      "Profile of RDD<id=183>\n",
      "============================================================\n",
      "         454662 function calls in 6083.709 seconds\n",
      "\n",
      "   Ordered by: internal time, cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "       27 4873.285  180.492 4873.285  180.492 {spark_fof.fof.fof.run}\n",
      "    39441  678.976    0.017  678.976    0.017 {method 'read' of 'file' objects}\n",
      "    15092  249.322    0.017  249.322    0.017 {method 'write' of 'file' objects}\n",
      "     7546  131.050    0.017  131.050    0.017 {cPickle.dumps}\n",
      "    19707   82.255    0.004   82.255    0.004 {cPickle.loads}\n",
      "       27   49.797    1.844   49.797    1.844 {numpy.core.multiarray.concatenate}\n",
      "       27    9.497    0.352    9.497    0.352 spark_fof_c.pyx:247(encode_gid (wrapper))\n",
      "       27    4.114    0.152 6083.707  225.322 serializers.py:259(dump_stream)\n",
      "     7573    3.312    0.000 5698.947    0.753 spark_fof.py:222(run_local_fof)\n",
      "    19734    0.448    0.000  762.228    0.039 serializers.py:155(_read_with_length)\n",
      "    19734    0.335    0.000  652.286    0.033 serializers.py:542(read_int)\n",
      "    30976    0.322    0.000    0.507    0.000 shape_base.py:9(atleast_1d)\n",
      "       27    0.208    0.008  812.835   30.105 shape_base.py:232(hstack)\n",
      "    96859    0.113    0.000    0.113    0.000 {len}\n",
      "    30976    0.104    0.000    0.140    0.000 numeric.py:484(asanyarray)\n",
      "    19707    0.098    0.000   82.353    0.004 serializers.py:421(loads)\n",
      "    19734    0.096    0.000  762.324    0.039 serializers.py:136(load_stream)\n",
      "     7546    0.090    0.000    0.298    0.000 serializers.py:549(write_int)\n",
      "    19734    0.084    0.000    0.084    0.000 {_struct.unpack}\n",
      "     7546    0.069    0.000    0.069    0.000 {_struct.pack}\n",
      "     7546    0.063    0.000  131.113    0.017 serializers.py:414(dumps)\n",
      "    30976    0.036    0.000    0.036    0.000 {numpy.core.multiarray.array}\n",
      "    38522    0.016    0.000    0.016    0.000 {method 'append' of 'list' objects}\n",
      "       27    0.007    0.000    0.015    0.001 shape_base.py:378(array_split)\n",
      "     7573    0.004    0.000    0.004    0.000 {method 'swapaxes' of 'numpy.ndarray' objects}\n",
      "     7573    0.003    0.000    0.006    0.000 fromnumeric.py:457(swapaxes)\n",
      "       27    0.001    0.000 6083.709  225.323 worker.py:165(process)\n",
      "       27    0.001    0.000    0.001    0.000 stringsource:341(__cinit__)\n",
      "       54    0.001    0.000    0.001    0.000 {range}\n",
      "       27    0.001    0.000    0.015    0.001 shape_base.py:427(split)\n",
      "       27    0.001    0.000    9.498    0.352 {spark_fof.spark_fof_c.encode_gid}\n",
      "       27    0.000    0.000    0.000    0.000 serializers.py:217(load_stream)\n",
      "       27    0.000    0.000    0.001    0.000 stringsource:643(memoryview_cwrapper)\n",
      "       27    0.000    0.000    0.000    0.000 stringsource:368(__dealloc__)\n",
      "       27    0.000    0.000    0.000    0.000 spark_fof.py:237(<lambda>)\n",
      "       27    0.000    0.000    0.000    0.000 serializers.py:220(_load_stream_without_unbatching)\n",
      "       27    0.000    0.000    0.000    0.000 stringsource:649(memoryview_check)\n",
      "       27    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "       27    0.000    0.000    0.000    0.000 {built-in method from_iterable}\n",
      "       27    0.000    0.000    0.000    0.000 {iter}\n",
      "\n",
      "\n",
      "============================================================\n",
      "Profile of RDD<id=184>\n",
      "============================================================\n",
      "         120219781 function calls (120219650 primitive calls) in 561.133 seconds\n",
      "\n",
      "   Ordered by: internal time, cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "  2498084  208.405    0.000  208.405    0.000 {cPickle.dumps}\n",
      "   957261  113.300    0.000  113.300    0.000 {method 'read' of 'file' objects}\n",
      "   477453   76.549    0.000   76.549    0.000 {cPickle.loads}\n",
      "     7546   25.195    0.003   25.195    0.003 {numpy.core.multiarray.where}\n",
      "  7673308   20.489    0.000   20.489    0.000 spark_fof.py:413(pid_gid)\n",
      "  4056381   14.422    0.000  230.830    0.000 rdd.py:1713(add_shuffle_key)\n",
      "       27   12.476    0.462  301.242   11.157 shuffle.py:229(mergeValues)\n",
      "  9052522   12.352    0.000   12.352    0.000 {method 'write' of 'file' objects}\n",
      "   469907   11.347    0.000   11.347    0.000 {zlib.compress}\n",
      "  4056354    6.847    0.000   23.812    0.000 serializers.py:143(_write_with_length)\n",
      "     2355    5.454    0.002    5.454    0.002 {method 'items' of 'dict' objects}\n",
      "   456337    5.084    0.000  104.816    0.000 serializers.py:259(dump_stream)\n",
      "  7673081    4.853    0.000    8.088    0.000 rdd.py:61(portable_hash)\n",
      "  6554438    3.586    0.000    3.586    0.000 {_struct.pack}\n",
      "  4526261    3.504    0.000   11.081    0.000 serializers.py:549(write_int)\n",
      "       27    3.322    0.123  257.964    9.554 serializers.py:132(dump_stream)\n",
      "  5480911    3.105    0.000    3.817    0.000 shuffle.py:253(_partition)\n",
      "     2320    2.975    0.001    2.975    0.001 {method 'clear' of 'dict' objects}\n",
      "  7673081    2.656    0.000    2.656    0.000 rdd.py:1859(createCombiner)\n",
      "  7673135    2.565    0.000    2.565    0.000 {isinstance}\n",
      "   469907    2.187    0.000    2.187    0.000 {zlib.decompress}\n",
      "  2498084    1.664    0.000  210.069    0.000 serializers.py:414(dumps)\n",
      "     2301    1.620    0.001   19.935    0.009 shuffle.py:263(mergeCombiners)\n",
      "  5480815    1.543    0.000   78.378    0.000 shuffle.py:343(_external_items)\n",
      " 11565572    1.498    0.000    1.498    0.000 {len}\n",
      "     7546    1.427    0.000   26.622    0.004 spark_fof.py:262(<lambda>)\n",
      " 13153992    1.382    0.000    1.382    0.000 {hash}\n",
      "     2301    1.292    0.001    1.292    0.001 {posix.remove}\n",
      "       39    1.114    0.029  111.954    2.871 shuffle.py:289(_spill)\n",
      "  7673308    1.042    0.000    1.042    0.000 {method 'append' of 'list' objects}\n",
      "   479781    0.995    0.000  194.727    0.000 serializers.py:155(_read_with_length)\n",
      "  2028177    0.847    0.000    1.806    0.000 serializers.py:538(pack_long)\n",
      "   469907    0.623    0.000   96.485    0.000 serializers.py:486(dumps)\n",
      "    15179    0.579    0.000    0.579    0.000 {open}\n",
      "   479781    0.565    0.000    9.983    0.000 serializers.py:542(read_int)\n",
      "    10523    0.561    0.000    1.328    0.000 _pslinux.py:1079(memory_info)\n",
      "       39    0.507    0.013    0.507    0.013 {gc.collect}\n",
      "   469907    0.501    0.000   15.643    0.000 serializers.py:489(loads)\n",
      "   479781    0.490    0.000  195.216    0.000 serializers.py:136(load_stream)\n",
      "  4056354    0.486    0.000    0.486    0.000 serializers.py:335(dumps)\n",
      "    10523    0.303    0.000    0.303    0.000 {method 'readline' of 'file' objects}\n",
      "   477453    0.300    0.000   76.850    0.000 serializers.py:421(loads)\n",
      "   477480    0.223    0.000    0.223    0.000 {_struct.unpack}\n",
      "    10523    0.139    0.000    1.576    0.000 shuffle.py:37(get_used_memory)\n",
      "   457525    0.112    0.000    0.112    0.000 {iter}\n",
      "     1121    0.090    0.000   21.537    0.019 shuffle.py:364(_merged_items)\n",
      "     1296    0.074    0.000    0.074    0.000 {posix.stat}\n",
      "       27    0.065    0.002  303.168   11.228 rdd.py:316(func)\n",
      "    11610    0.045    0.000    0.061    0.000 posixpath.py:61(join)\n",
      "    10523    0.037    0.000    1.400    0.000 __init__.py:985(memory_info)\n",
      "     7662    0.036    0.000    0.036    0.000 {method 'keys' of 'dict' objects}\n",
      "    10550    0.035    0.000    1.378    0.000 _pslinux.py:926(wrapper)\n",
      "       77    0.027    0.000    0.027    0.000 {posix.mkdir}\n",
      "    10604    0.025    0.000    0.025    0.000 {method 'split' of 'str' objects}\n",
      "    10577    0.024    0.000    0.400    0.000 _pslinux.py:140(open_binary)\n",
      "     4641    0.024    0.000    0.051    0.000 shuffle.py:217(_get_spill_dir)\n",
      "    10523    0.022    0.000    0.042    0.000 <string>:8(__new__)\n",
      "    10523    0.012    0.000    0.012    0.000 {hasattr}\n",
      "     1121    0.012    0.000    0.012    0.000 {method 'close' of 'file' objects}\n",
      "     1121    0.011    0.000    0.011    0.000 {method 'tell' of 'file' objects}\n",
      "    12420    0.010    0.000    0.010    0.000 {method 'startswith' of 'str' objects}\n",
      "       27    0.009    0.000    0.010    0.000 _pslinux.py:501(boot_time)\n",
      "     2328    0.008    0.000    0.013    0.000 serializers.py:217(load_stream)\n",
      "    10550    0.008    0.000    0.008    0.000 {posix.getpid}\n",
      "     1180    0.007    0.000    0.079    0.000 genericpath.py:55(getsize)\n",
      "     1141    0.007    0.000    0.161    0.000 shuffle.py:221(_next_limit)\n",
      "    11664    0.005    0.000    0.005    0.000 {method 'endswith' of 'str' objects}\n",
      "     2319    0.005    0.000    0.005    0.000 {range}\n",
      "     2328    0.003    0.000    0.003    0.000 serializers.py:220(_load_stream_without_unbatching)\n",
      "       58    0.003    0.000    0.003    0.000 {posix.rmdir}\n",
      "     1180    0.002    0.000    0.002    0.000 {max}\n",
      "       27    0.002    0.000    0.005    0.000 _pslinux.py:953(_parse_stat_file)\n",
      "     2355    0.002    0.000    0.002    0.000 {built-in method from_iterable}\n",
      "      444    0.001    0.000    0.001    0.000 {built-in method __new__ of type object at 0x2ba5bfd94da0}\n",
      "       58    0.001    0.000    0.001    0.000 {posix.listdir}\n",
      "      104    0.001    0.000    0.001    0.000 {method 'rfind' of 'str' objects}\n",
      "      426    0.001    0.000    0.001    0.000 {built-in method __new__ of type object at 0x2add7882fda0}\n",
      "      567    0.001    0.000    0.001    0.000 {built-in method __new__ of type object at 0x2aadf0869da0}\n",
      "      583    0.001    0.000    0.001    0.000 {built-in method __new__ of type object at 0x2abb34c98da0}\n",
      "      430    0.001    0.000    0.001    0.000 {built-in method __new__ of type object at 0x2afec48fbda0}\n",
      "      361    0.001    0.000    0.001    0.000 {built-in method __new__ of type object at 0x2b0396049da0}\n",
      "      366    0.001    0.000    0.001    0.000 {built-in method __new__ of type object at 0x2b4675e30da0}\n",
      "      429    0.001    0.000    0.001    0.000 {built-in method __new__ of type object at 0x2b8783cf2da0}\n",
      "       27    0.001    0.000    0.017    0.001 __init__.py:366(_init)\n",
      "      443    0.001    0.000    0.001    0.000 {built-in method __new__ of type object at 0x2aee25501da0}\n",
      "      450    0.001    0.000    0.001    0.000 {built-in method __new__ of type object at 0x2b05465e0da0}\n",
      "      416    0.001    0.000    0.001    0.000 {built-in method __new__ of type object at 0x2b27fdce8da0}\n",
      "      227    0.001    0.000    0.001    0.000 rdd.py:1862(mergeValue)\n",
      "      438    0.001    0.000    0.001    0.000 {built-in method __new__ of type object at 0x2ae125b7fda0}\n",
      "       27    0.001    0.000  303.103   11.226 rdd.py:1874(combine)\n",
      "      315    0.001    0.000    0.001    0.000 {built-in method __new__ of type object at 0x2b95e4166da0}\n",
      "      440    0.001    0.000    0.001    0.000 {built-in method __new__ of type object at 0x2b5d214e0da0}\n",
      "      319    0.001    0.000    0.001    0.000 {built-in method __new__ of type object at 0x2ab23fcb2da0}\n",
      "      414    0.001    0.000    0.001    0.000 {built-in method __new__ of type object at 0x2afe73f2cda0}\n",
      "      452    0.001    0.000    0.001    0.000 {built-in method __new__ of type object at 0x2b575d836da0}\n",
      "      330    0.001    0.000    0.001    0.000 {built-in method __new__ of type object at 0x2b99d2bf8da0}\n",
      "       97    0.001    0.000    0.001    0.000 {posix.lstat}\n",
      "      116    0.001    0.000    0.003    0.000 genericpath.py:23(exists)\n",
      "       27    0.001    0.000    0.003    0.000 shuffle.py:196(__init__)\n",
      "      410    0.001    0.000    0.001    0.000 {built-in method __new__ of type object at 0x2b9127db1da0}\n",
      "      309    0.001    0.000    0.001    0.000 {built-in method __new__ of type object at 0x2b37bb7dcda0}\n",
      "      298    0.001    0.000    0.001    0.000 {built-in method __new__ of type object at 0x2b79471b1da0}\n",
      "    58/19    0.001    0.000    0.006    0.000 shutil.py:210(rmtree)\n",
      "      414    0.001    0.000    0.001    0.000 {built-in method __new__ of type object at 0x2b2e0d100da0}\n",
      "      373    0.001    0.000    0.001    0.000 {built-in method __new__ of type object at 0x2b3151fa1da0}\n",
      "      261    0.001    0.000    0.001    0.000 {built-in method __new__ of type object at 0x2b56f23aeda0}\n",
      "      396    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2b0747ff7da0}\n",
      "       27    0.000    0.000    0.001    0.000 shuffle.py:69(_get_local_dirs)\n",
      "      211    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2b3700449da0}\n",
      "    77/39    0.000    0.000    0.030    0.001 os.py:136(makedirs)\n",
      "       27    0.000    0.000    0.000    0.000 _pslinux.py:947(__init__)\n",
      "       77    0.000    0.000    0.002    0.000 posixpath.py:82(split)\n",
      "      228    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2b1704470da0}\n",
      "       27    0.000    0.000    0.015    0.001 _pslinux.py:1068(create_time)\n",
      "    81/27    0.000    0.000  303.168   11.228 rdd.py:2370(pipeline_func)\n",
      "       27    0.000    0.000  561.133   20.783 worker.py:165(process)\n",
      "       27    0.000    0.000    0.001    0.000 shuffle.py:134(_compressed_serializer)\n",
      "       27    0.000    0.000    1.857    0.069 shuffle.py:337(items)\n",
      "       58    0.000    0.000    0.001    0.000 posixpath.py:132(islink)\n",
      "       27    0.000    0.000    0.000    0.000 serializers.py:255(__init__)\n",
      "       54    0.000    0.000    0.000    0.000 serializers.py:127(__init__)\n",
      "       27    0.000    0.000    0.000    0.000 UserDict.py:103(__contains__)\n",
      "       27    0.000    0.000    0.000    0.000 UserDict.py:91(get)\n",
      "       19    0.000    0.000    0.006    0.000 shuffle.py:409(_cleanup)\n",
      "       27    0.000    0.000    0.000    0.000 serializers.py:481(__init__)\n",
      "       27    0.000    0.000    0.000    0.000 UserDict.py:35(__getitem__)\n",
      "       27    0.000    0.000    0.015    0.001 __init__.py:644(create_time)\n",
      "       54    0.000    0.000    0.000    0.000 _pslinux.py:165(get_procfs_path)\n",
      "       27    0.000    0.000    0.017    0.001 __init__.py:363(__init__)\n",
      "       77    0.000    0.000    0.000    0.000 {method 'rstrip' of 'str' objects}\n",
      "       58    0.000    0.000    0.000    0.000 stat.py:55(S_ISLNK)\n",
      "       27    0.000    0.000    0.000    0.000 rdd.py:303(func)\n",
      "       27    0.000    0.000    0.000    0.000 {min}\n",
      "       97    0.000    0.000    0.000    0.000 stat.py:24(S_IFMT)\n",
      "       27    0.000    0.000    0.000    0.000 shuffle.py:118(__init__)\n",
      "       39    0.000    0.000    0.000    0.000 stat.py:40(S_ISDIR)\n",
      "       27    0.000    0.000    0.000    0.000 serializers.py:190(__init__)\n",
      "       19    0.000    0.000    0.000    0.000 {any}\n",
      "       19    0.000    0.000    0.000    0.000 {method 'extend' of 'list' objects}\n",
      "       54    0.000    0.000    0.000    0.000 {id}\n",
      "       27    0.000    0.000    0.000    0.000 {method 'find' of 'str' objects}\n",
      "       27    0.000    0.000    0.000    0.000 __init__.py:528(pid)\n",
      "       27    0.000    0.000    0.000    0.000 {method 'strip' of 'str' objects}\n",
      "       27    0.000    0.000    0.000    0.000 rdd.py:288(func)\n",
      "       27    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "\n",
      "\n",
      "============================================================\n",
      "Profile of RDD<id=188>\n",
      "============================================================\n",
      "         114189731 function calls (114181977 primitive calls) in 192.114 seconds\n",
      "\n",
      "   Ordered by: internal time, cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "   175269   56.358    0.000   56.358    0.000 {cPickle.dumps}\n",
      "  2199666   36.150    0.000   36.150    0.000 {cPickle.loads}\n",
      "118895/112287   13.754    0.000  127.745    0.001 serializers.py:259(dump_stream)\n",
      "    13486   11.074    0.001   82.817    0.006 shuffle.py:263(mergeCombiners)\n",
      "  5733284   11.006    0.000   16.176    0.000 {sorted}\n",
      "   171489    6.745    0.000    6.745    0.000 {zlib.compress}\n",
      "  5732984    5.456    0.000    9.775    0.000 rdd.py:1914(<lambda>)\n",
      "  4412829    4.443    0.000    4.443    0.000 {method 'read' of 'file' objects}\n",
      "  2213152    4.196    0.000   51.394    0.000 serializers.py:155(_read_with_length)\n",
      "  5732984    3.642    0.000    4.319    0.000 resultiterable.py:30(__init__)\n",
      "  5732984    2.983    0.000    2.983    0.000 spark_fof.py:267(<lambda>)\n",
      "      224    2.770    0.012    2.770    0.012 {gc.collect}\n",
      " 21838535    2.541    0.000    2.541    0.000 {len}\n",
      "  2213152    2.345    0.000    5.410    0.000 serializers.py:542(read_int)\n",
      "  7673081    2.111    0.000    2.942    0.000 shuffle.py:704(_object_size)\n",
      "  5732984    2.003    0.000    2.887    0.000 resultiterable.py:35(__iter__)\n",
      "  2213152    1.828    0.000   53.222    0.000 serializers.py:136(load_stream)\n",
      "  3181510    1.826    0.000    2.234    0.000 shuffle.py:253(_partition)\n",
      "  5732984    1.716    0.000    2.283    0.000 resultiterable.py:38(__len__)\n",
      "  5732984    1.654    0.000   17.780    0.000 spark_fof.py:266(<lambda>)\n",
      "   171489    1.513    0.000    1.513    0.000 {zlib.decompress}\n",
      "   350538    1.293    0.000    1.293    0.000 {method 'write' of 'file' objects}\n",
      "  2199666    1.219    0.000   37.369    0.000 serializers.py:421(loads)\n",
      "    29938    1.118    0.000    1.118    0.000 {open}\n",
      "    13328    1.057    0.000    1.057    0.000 {method 'clear' of 'dict' objects}\n",
      "    13481    1.037    0.000    1.037    0.000 {method 'items' of 'dict' objects}\n",
      "  2199936    0.975    0.000    0.975    0.000 {_struct.unpack}\n",
      "  5732984    0.967    0.000    0.967    0.000 rdd.py:1556(<lambda>)\n",
      "  5852037    0.926    0.000    0.926    0.000 {iter}\n",
      "      224    0.924    0.004   51.612    0.230 shuffle.py:707(_spill)\n",
      "  1940097    0.919    0.000    1.240    0.000 rdd.py:1866(mergeCombiners)\n",
      "    13216    0.905    0.000    0.905    0.000 {posix.remove}\n",
      "  2377271    0.649    0.000   52.784    0.000 shuffle.py:343(_external_items)\n",
      "  3181510    0.408    0.000    0.408    0.000 {hash}\n",
      "     6608    0.340    0.000   10.904    0.002 shuffle.py:766(_merged_items)\n",
      "  1940209    0.322    0.000    0.322    0.000 {method 'extend' of 'list' objects}\n",
      "   171489    0.309    0.000   41.641    0.000 serializers.py:486(dumps)\n",
      "    20384    0.286    0.000    0.286    0.000 {posix.stat}\n",
      "   175269    0.261    0.000    1.031    0.000 serializers.py:549(write_int)\n",
      "   175269    0.241    0.000    0.241    0.000 {_struct.pack}\n",
      "   171489    0.215    0.000    7.314    0.000 serializers.py:489(loads)\n",
      "     3484    0.207    0.000    0.471    0.000 _pslinux.py:1079(memory_info)\n",
      "    93230    0.195    0.000    0.283    0.000 posixpath.py:61(join)\n",
      "   175269    0.169    0.000   56.527    0.000 serializers.py:414(dumps)\n",
      "    39872    0.125    0.000    0.274    0.000 shuffle.py:217(_get_spill_dir)\n",
      "   113898    0.119    0.000    0.158    0.000 serializers.py:237(_batched)\n",
      "     3484    0.109    0.000    0.109    0.000 {method 'readline' of 'file' objects}\n",
      "     6608    0.069    0.000    0.069    0.000 {method 'tell' of 'file' objects}\n",
      "     6608    0.066    0.000    0.066    0.000 {method 'close' of 'file' objects}\n",
      "    94078    0.058    0.000    0.058    0.000 {method 'startswith' of 'str' objects}\n",
      "    19824    0.054    0.000    0.330    0.000 genericpath.py:55(getsize)\n",
      "   128964    0.054    0.000    0.054    0.000 {range}\n",
      "    19824    0.043    0.000    0.276    0.000 shuffle.py:767(<genexpr>)\n",
      "      270    0.042    0.000   73.673    0.273 rdd.py:316(func)\n",
      "     3484    0.039    0.000    0.553    0.000 shuffle.py:37(get_used_memory)\n",
      "    13486    0.035    0.000    0.054    0.000 serializers.py:217(load_stream)\n",
      "    93770    0.030    0.000    0.030    0.000 {method 'endswith' of 'str' objects}\n",
      "      336    0.027    0.000    0.027    0.000 {posix.mkdir}\n",
      "      336    0.015    0.000    0.015    0.000 {posix.rmdir}\n",
      "     6608    0.014    0.000    0.290    0.000 {sum}\n",
      "    13486    0.013    0.000    0.013    0.000 serializers.py:220(_load_stream_without_unbatching)\n",
      "     3495    0.011    0.000    0.494    0.000 _pslinux.py:926(wrapper)\n",
      "     3484    0.011    0.000    0.494    0.000 __init__.py:985(memory_info)\n",
      "     3484    0.009    0.000    0.016    0.000 <string>:8(__new__)\n",
      "     3776    0.009    0.000    0.009    0.000 {method 'split' of 'str' objects}\n",
      "      336    0.009    0.000    0.009    0.000 {posix.listdir}\n",
      "       11    0.009    0.001    0.009    0.001 _pslinux.py:501(boot_time)\n",
      "     3506    0.007    0.000    0.132    0.000 _pslinux.py:140(open_binary)\n",
      "     5206    0.007    0.000    2.174    0.000 serializers.py:214(dump_stream)\n",
      "    13756    0.006    0.000    0.006    0.000 {built-in method from_iterable}\n",
      "      270    0.005    0.000    0.013    0.000 shuffle.py:69(_get_local_dirs)\n",
      "      270    0.005    0.000    0.024    0.000 shuffle.py:196(__init__)\n",
      "     3754    0.004    0.000    0.004    0.000 {posix.getpid}\n",
      "      560    0.004    0.000    0.004    0.000 {posix.lstat}\n",
      "  336/112    0.003    0.000    0.034    0.000 shutil.py:210(rmtree)\n",
      "      270    0.003    0.000    0.566    0.002 shuffle.py:337(items)\n",
      "     3484    0.003    0.000    0.003    0.000 {hasattr}\n",
      " 1080/270    0.003    0.000   73.679    0.273 rdd.py:2370(pipeline_func)\n",
      "      560    0.003    0.000    0.013    0.000 genericpath.py:23(exists)\n",
      "      270    0.003    0.000   73.631    0.273 rdd.py:1882(groupByKey)\n",
      "      270    0.003    0.000  192.114    0.712 worker.py:165(process)\n",
      "      270    0.002    0.000    0.006    0.000 shuffle.py:134(_compressed_serializer)\n",
      "      270    0.002    0.000    0.003    0.000 UserDict.py:91(get)\n",
      "  336/224    0.002    0.000    0.035    0.000 os.py:136(makedirs)\n",
      "      336    0.002    0.000    0.003    0.000 posixpath.py:82(split)\n",
      "      810    0.001    0.000    0.001    0.000 rdd.py:288(func)\n",
      "      270    0.001    0.000    0.002    0.000 rdd.py:303(func)\n",
      "      270    0.001    0.000    0.002    0.000 serializers.py:481(__init__)\n",
      "      336    0.001    0.000    0.004    0.000 posixpath.py:132(islink)\n",
      "      540    0.001    0.000    0.001    0.000 serializers.py:127(__init__)\n",
      "      270    0.001    0.000    0.001    0.000 serializers.py:255(__init__)\n",
      "      336    0.001    0.000    0.001    0.000 {max}\n",
      "      590    0.001    0.000    0.003    0.000 serializers.py:243(load_stream)\n",
      "       11    0.001    0.000    0.002    0.000 _pslinux.py:953(_parse_stat_file)\n",
      "      119    0.001    0.000    0.001    0.000 {built-in method __new__ of type object at 0x2afe73f2cda0}\n",
      "      112    0.001    0.000    0.034    0.000 shuffle.py:409(_cleanup)\n",
      "      270    0.000    0.000    0.000    0.000 UserDict.py:103(__contains__)\n",
      "       11    0.000    0.000    0.012    0.001 __init__.py:366(_init)\n",
      "      347    0.000    0.000    0.000    0.000 {method 'rfind' of 'str' objects}\n",
      "      336    0.000    0.000    0.000    0.000 {method 'rstrip' of 'str' objects}\n",
      "      112    0.000    0.000    0.007    0.000 shuffle.py:221(_next_limit)\n",
      "      336    0.000    0.000    0.001    0.000 stat.py:55(S_ISLNK)\n",
      "      140    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2b0396049da0}\n",
      "      140    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2b95e4166da0}\n",
      "      140    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2b4675e30da0}\n",
      "      270    0.000    0.000    0.000    0.000 shuffle.py:118(__init__)\n",
      "      119    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2add7882fda0}\n",
      "      270    0.000    0.000    0.000    0.000 UserDict.py:35(__getitem__)\n",
      "      140    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2b79471b1da0}\n",
      "      140    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2b37bb7dcda0}\n",
      "      140    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2ab23fcb2da0}\n",
      "      140    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2b99d2bf8da0}\n",
      "      140    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2b3700449da0}\n",
      "      560    0.000    0.000    0.000    0.000 stat.py:24(S_IFMT)\n",
      "      150    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2b1704470da0}\n",
      "      150    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2b56f23aeda0}\n",
      "      160    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2b0747ff7da0}\n",
      "      275    0.000    0.000    0.000    0.000 serializers.py:190(__init__)\n",
      "      102    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2ba5bfd94da0}\n",
      "      119    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2aadf0869da0}\n",
      "      136    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2ae125b7fda0}\n",
      "      119    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2b05465e0da0}\n",
      "      119    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2abb34c98da0}\n",
      "      119    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2b27fdce8da0}\n",
      "      119    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2afec48fbda0}\n",
      "      224    0.000    0.000    0.000    0.000 stat.py:40(S_ISDIR)\n",
      "      119    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2b8783cf2da0}\n",
      "      119    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2b5d214e0da0}\n",
      "      119    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2aee25501da0}\n",
      "      119    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2b575d836da0}\n",
      "      119    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2b9127db1da0}\n",
      "      286    0.000    0.000    0.000    0.000 {isinstance}\n",
      "      540    0.000    0.000    0.000    0.000 {id}\n",
      "       11    0.000    0.000    0.012    0.001 _pslinux.py:1068(create_time)\n",
      "      119    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2b2e0d100da0}\n",
      "      270    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "      119    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2b3151fa1da0}\n",
      "       11    0.000    0.000    0.000    0.000 _pslinux.py:947(__init__)\n",
      "       22    0.000    0.000    0.000    0.000 _pslinux.py:165(get_procfs_path)\n",
      "        5    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}\n",
      "      112    0.000    0.000    0.000    0.000 {any}\n",
      "       11    0.000    0.000    0.013    0.001 __init__.py:363(__init__)\n",
      "        5    0.000    0.000    0.000    0.000 shuffle.py:699(flattened_serializer)\n",
      "       11    0.000    0.000    0.012    0.001 __init__.py:644(create_time)\n",
      "       11    0.000    0.000    0.000    0.000 {method 'strip' of 'str' objects}\n",
      "        5    0.000    0.000    0.000    0.000 serializers.py:234(__init__)\n",
      "       11    0.000    0.000    0.000    0.000 {method 'find' of 'str' objects}\n",
      "       11    0.000    0.000    0.000    0.000 __init__.py:528(pid)\n",
      "\n",
      "\n",
      "============================================================\n",
      "Profile of RDD<id=189>\n",
      "============================================================\n",
      "         18029285 function calls (18029069 primitive calls) in 1242.597 seconds\n",
      "\n",
      "   Ordered by: internal time, cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "  2185639  348.947    0.000  348.947    0.000 spark_fof.py:367(<genexpr>)\n",
      "     7546  225.314    0.030  227.855    0.030 spark_fof.py:383(<lambda>)\n",
      "     7546  149.119    0.020  149.119    0.020 spark_fof_c.pyx:84(remap_gid_partition_cython)\n",
      "       54  105.647    1.956  105.647    1.956 {cPickle.load}\n",
      "   102584  102.340    0.001  102.340    0.001 {method 'read' of 'file' objects}\n",
      "    49672   96.612    0.002   96.612    0.002 {cPickle.loads}\n",
      "       27   85.493    3.166   85.493    3.166 {method 'sort' of 'numpy.ndarray' objects}\n",
      "       81   43.454    0.536   43.454    0.536 {numpy.core.multiarray.concatenate}\n",
      "    68794   20.419    0.000   20.419    0.000 {cPickle.dumps}\n",
      "       27   13.700    0.507   13.700    0.507 {method 'flatten' of 'numpy.ndarray' objects}\n",
      "       27   13.320    0.493  127.408    4.719 arraysetops.py:96(unique)\n",
      "       27    6.588    0.244    6.588    0.244 {method 'nonzero' of 'numpy.ndarray' objects}\n",
      "       27    4.051    0.150  797.087   29.522 spark_fof.py:363(count_groups_partition)\n",
      "       27    3.541    0.131    3.541    0.131 function_base.py:1507(diff)\n",
      "       27    3.175    0.118  800.262   29.639 spark_fof.py:386(<lambda>)\n",
      "       27    2.691    0.100  357.863   13.254 shuffle.py:229(mergeValues)\n",
      "     7546    2.540    0.000    2.540    0.000 {numpy.core.multiarray.where}\n",
      "  2185612    1.649    0.000    2.989    0.000 spark_fof.py:389(<lambda>)\n",
      "   190924    1.536    0.000    1.536    0.000 {method 'write' of 'file' objects}\n",
      "    42126    1.395    0.000    1.395    0.000 {zlib.compress}\n",
      "    53363    1.268    0.000   28.960    0.001 rdd.py:1713(add_shuffle_key)\n",
      "       54    1.034    0.019    1.034    0.019 {gc.collect}\n",
      "  2185666    0.980    0.000  106.997    0.000 broadcast.py:92(value)\n",
      "   926081    0.825    0.000    1.256    0.000 rdd.py:61(portable_hash)\n",
      "   926081    0.464    0.000    0.574    0.000 shuffle.py:253(_partition)\n",
      "    28593    0.455    0.000   11.835    0.000 serializers.py:259(dump_stream)\n",
      "    42126    0.401    0.000    0.401    0.000 {zlib.decompress}\n",
      "  2190184    0.364    0.000    0.364    0.000 {hasattr}\n",
      "   926135    0.354    0.000    0.354    0.000 {isinstance}\n",
      "    10998    0.350    0.000    0.350    0.000 {open}\n",
      "    52885    0.315    0.000  200.077    0.004 serializers.py:155(_read_with_length)\n",
      "     3186    0.302    0.000    0.302    0.000 {posix.remove}\n",
      "     3186    0.298    0.000    3.040    0.001 shuffle.py:263(mergeCombiners)\n",
      "     7573    0.250    0.000  347.326    0.046 spark_fof.py:338(remap_partition)\n",
      "     1593    0.188    0.000    3.708    0.002 shuffle.py:364(_merged_items)\n",
      "  1852162    0.187    0.000    0.187    0.000 {hash}\n",
      "   926108    0.180    0.000   15.266    0.000 shuffle.py:343(_external_items)\n",
      "       54    0.180    0.003   13.523    0.250 shuffle.py:289(_spill)\n",
      "     7546    0.176    0.000    0.176    0.000 stringsource:341(__cinit__)\n",
      "    52885    0.166    0.000  200.243    0.004 serializers.py:136(load_stream)\n",
      "   926081    0.160    0.000    0.160    0.000 {method 'append' of 'list' objects}\n",
      "     4518    0.151    0.000    0.352    0.000 _pslinux.py:1079(memory_info)\n",
      "    53336    0.136    0.000    1.547    0.000 serializers.py:143(_write_with_length)\n",
      "     7546    0.135    0.000  149.574    0.020 {spark_fof.spark_fof_c.remap_gid_partition_cython}\n",
      "    52885    0.131    0.000    1.029    0.000 serializers.py:542(read_int)\n",
      "    49699    0.113    0.000    0.113    0.000 {_struct.unpack}\n",
      "   926081    0.110    0.000    0.110    0.000 rdd.py:1574(<lambda>)\n",
      "    95462    0.103    0.000    0.360    0.000 serializers.py:549(write_int)\n",
      "   122130    0.100    0.000    0.100    0.000 {_struct.pack}\n",
      "     4518    0.093    0.000    0.093    0.000 {method 'readline' of 'file' objects}\n",
      "     3213    0.092    0.000    0.092    0.000 {method 'clear' of 'dict' objects}\n",
      "    49672    0.085    0.000   96.697    0.002 serializers.py:421(loads)\n",
      "     3240    0.084    0.000    0.084    0.000 {method 'items' of 'dict' objects}\n",
      "    16065    0.071    0.000    0.088    0.000 posixpath.py:61(join)\n",
      "   274120    0.068    0.000    0.068    0.000 {len}\n",
      "     7546    0.067    0.000  149.197    0.020 spark_fof_c.pyx:84(remap_gid_partition_cython (wrapper))\n",
      "    42126    0.062    0.000   11.023    0.000 serializers.py:486(dumps)\n",
      "     7546    0.058    0.000    0.234    0.000 stringsource:643(memoryview_cwrapper)\n",
      "       27    0.058    0.002   30.565    1.132 serializers.py:132(dump_stream)\n",
      "    68794    0.057    0.000   20.477    0.000 serializers.py:414(dumps)\n",
      "    42126    0.052    0.000    2.226    0.000 serializers.py:489(loads)\n",
      "     1755    0.044    0.000    0.044    0.000 {posix.stat}\n",
      "     4518    0.030    0.000    0.425    0.000 shuffle.py:37(get_used_memory)\n",
      "     6426    0.025    0.000    0.055    0.000 shuffle.py:217(_get_spill_dir)\n",
      "      108    0.024    0.000    0.024    0.000 {posix.mkdir}\n",
      "     1593    0.016    0.000    0.016    0.000 {method 'close' of 'file' objects}\n",
      "     1593    0.016    0.000    0.016    0.000 {method 'tell' of 'file' objects}\n",
      "    26668    0.014    0.000    0.032    0.000 serializers.py:538(pack_long)\n",
      "     7546    0.012    0.000    0.012    0.000 stringsource:368(__dealloc__)\n",
      "    16875    0.012    0.000    0.012    0.000 {method 'startswith' of 'str' objects}\n",
      "     4545    0.011    0.000    0.378    0.000 _pslinux.py:926(wrapper)\n",
      "     4518    0.010    0.000    0.373    0.000 __init__.py:985(memory_info)\n",
      "     3213    0.009    0.000    0.014    0.000 serializers.py:217(load_stream)\n",
      "       27    0.009    0.000    0.010    0.000 _pslinux.py:501(boot_time)\n",
      "    30186    0.008    0.000    0.008    0.000 {iter}\n",
      "     1620    0.008    0.000    0.202    0.000 shuffle.py:221(_next_limit)\n",
      "    53336    0.007    0.000    0.007    0.000 serializers.py:335(dumps)\n",
      "     7546    0.007    0.000    0.007    0.000 stringsource:649(memoryview_check)\n",
      "     4599    0.007    0.000    0.007    0.000 {method 'split' of 'str' objects}\n",
      "     4518    0.006    0.000    0.012    0.000 <string>:8(__new__)\n",
      "     4572    0.006    0.000    0.091    0.000 _pslinux.py:140(open_binary)\n",
      "    16119    0.006    0.000    0.006    0.000 {method 'endswith' of 'str' objects}\n",
      "     1593    0.006    0.000    0.047    0.000 genericpath.py:55(getsize)\n",
      "       54    0.006    0.000  105.656    1.957 broadcast.py:82(load)\n",
      "     3294    0.005    0.000    0.005    0.000 {range}\n",
      "       81    0.004    0.000    0.004    0.000 {posix.rmdir}\n",
      "     3213    0.003    0.000    0.003    0.000 serializers.py:220(_load_stream_without_unbatching)\n",
      "     1674    0.003    0.000    0.003    0.000 {max}\n",
      "     4545    0.002    0.000    0.002    0.000 {posix.getpid}\n",
      "     3213    0.002    0.000    0.002    0.000 {built-in method from_iterable}\n",
      "       81    0.002    0.000    0.002    0.000 {posix.listdir}\n",
      "       27    0.002    0.000    0.005    0.000 _pslinux.py:953(_parse_stat_file)\n",
      "      162    0.001    0.000    0.004    0.000 genericpath.py:23(exists)\n",
      "      962    0.001    0.000    0.001    0.000 {method 'keys' of 'dict' objects}\n",
      "       27    0.001    0.000  357.868   13.254 rdd.py:1790(combineLocally)\n",
      "      135    0.001    0.000    0.001    0.000 {posix.lstat}\n",
      "       27    0.001    0.000    0.004    0.000 shuffle.py:196(__init__)\n",
      "   135/27    0.001    0.000 1212.031   44.890 rdd.py:2370(pipeline_func)\n",
      "    81/27    0.001    0.000    0.008    0.000 shutil.py:210(rmtree)\n",
      "       27    0.001    0.000    0.016    0.001 __init__.py:366(_init)\n",
      "       27    0.001    0.000    0.002    0.000 shuffle.py:69(_get_local_dirs)\n",
      "      108    0.001    0.000    0.001    0.000 posixpath.py:82(split)\n",
      "   108/54    0.001    0.000    0.026    0.000 os.py:136(makedirs)\n",
      "       54    0.001    0.000    0.001    0.000 numeric.py:484(asanyarray)\n",
      "       27    0.001    0.000    6.588    0.244 fromnumeric.py:1490(nonzero)\n",
      "      108    0.001    0.000 1212.030   11.223 rdd.py:316(func)\n",
      "       27    0.000    0.000   53.899    1.996 spark_fof.py:347(<lambda>)\n",
      "       27    0.000    0.000    0.002    0.000 shuffle.py:134(_compressed_serializer)\n",
      "       27    0.000    0.000    0.015    0.001 _pslinux.py:1068(create_time)\n",
      "       27    0.000    0.000    0.000    0.000 serializers.py:481(__init__)\n",
      "       27    0.000    0.000    0.000    0.000 _pslinux.py:947(__init__)\n",
      "      184    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2add7882fda0}\n",
      "      169    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2ba5bfd94da0}\n",
      "       54    0.000    0.000    0.000    0.000 serializers.py:127(__init__)\n",
      "       27    0.000    0.000 1242.597   46.022 worker.py:165(process)\n",
      "      181    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2b3700449da0}\n",
      "       27    0.000    0.000    0.000    0.000 serializers.py:255(__init__)\n",
      "       81    0.000    0.000    0.001    0.000 posixpath.py:132(islink)\n",
      "      181    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2ab23fcb2da0}\n",
      "      181    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2b4675e30da0}\n",
      "      168    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2afec48fbda0}\n",
      "      188    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2b99d2bf8da0}\n",
      "      187    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2b95e4166da0}\n",
      "      181    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2abb34c98da0}\n",
      "      185    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2ae125b7fda0}\n",
      "       27    0.000    0.000    0.000    0.000 rdd.py:366(func)\n",
      "      171    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2afe73f2cda0}\n",
      "      163    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2b27fdce8da0}\n",
      "      170    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2b8783cf2da0}\n",
      "      168    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2b37bb7dcda0}\n",
      "      165    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2b56f23aeda0}\n",
      "       27    0.000    0.000    0.000    0.000 UserDict.py:91(get)\n",
      "      180    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2b3151fa1da0}\n",
      "      169    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2aee25501da0}\n",
      "      166    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2b79471b1da0}\n",
      "      160    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2b1704470da0}\n",
      "      150    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2b0396049da0}\n",
      "       27    0.000    0.000    0.000    0.000 rdd.py:288(func)\n",
      "      164    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2b0747ff7da0}\n",
      "      135    0.000    0.000    0.000    0.000 {method 'rfind' of 'str' objects}\n",
      "       27    0.000    0.000    0.000    0.000 UserDict.py:103(__contains__)\n",
      "      148    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2b5d214e0da0}\n",
      "       27    0.000    0.000    0.000    0.000 shuffle.py:118(__init__)\n",
      "      151    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2b2e0d100da0}\n",
      "       27    0.000    0.000    0.016    0.001 __init__.py:363(__init__)\n",
      "      156    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2aadf0869da0}\n",
      "      153    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2b575d836da0}\n",
      "      147    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2b9127db1da0}\n",
      "       27    0.000    0.000    0.008    0.000 shuffle.py:409(_cleanup)\n",
      "      132    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2b05465e0da0}\n",
      "       54    0.000    0.000    0.000    0.000 {numpy.core.multiarray.array}\n",
      "       27    0.000    0.000    0.000    0.000 UserDict.py:35(__getitem__)\n",
      "       27    0.000    0.000    0.015    0.001 __init__.py:644(create_time)\n",
      "       54    0.000    0.000    0.000    0.000 _pslinux.py:165(get_procfs_path)\n",
      "      108    0.000    0.000    0.000    0.000 {method 'rstrip' of 'str' objects}\n",
      "       81    0.000    0.000    0.000    0.000 stat.py:55(S_ISLNK)\n",
      "       27    0.000    0.000    0.000    0.000 shuffle.py:337(items)\n",
      "       27    0.000    0.000    0.000    0.000 {min}\n",
      "      135    0.000    0.000    0.000    0.000 stat.py:24(S_IFMT)\n",
      "       27    0.000    0.000    0.000    0.000 serializers.py:190(__init__)\n",
      "       54    0.000    0.000    0.000    0.000 stat.py:40(S_ISDIR)\n",
      "       54    0.000    0.000    0.000    0.000 {id}\n",
      "       54    0.000    0.000    0.000    0.000 {gc.enable}\n",
      "       27    0.000    0.000    0.000    0.000 {any}\n",
      "       27    0.000    0.000    0.000    0.000 {method 'extend' of 'list' objects}\n",
      "       27    0.000    0.000    0.000    0.000 {method 'strip' of 'str' objects}\n",
      "       27    0.000    0.000    0.000    0.000 __init__.py:528(pid)\n",
      "       27    0.000    0.000    0.000    0.000 {method 'find' of 'str' objects}\n",
      "       54    0.000    0.000    0.000    0.000 {gc.disable}\n",
      "       27    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "\n",
      "\n",
      "============================================================\n",
      "Profile of RDD<id=193>\n",
      "============================================================\n",
      "         8894159 function calls (8894105 primitive calls) in 1110.035 seconds\n",
      "\n",
      "   Ordered by: internal time, cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "  2185639  349.456    0.000  349.456    0.000 spark_fof.py:367(<genexpr>)\n",
      "     7546  226.176    0.030  228.664    0.030 spark_fof.py:383(<lambda>)\n",
      "     7546  148.740    0.020  148.740    0.020 spark_fof_c.pyx:84(remap_gid_partition_cython)\n",
      "    15119  101.606    0.007  101.606    0.007 {method 'read' of 'file' objects}\n",
      "     7546   93.840    0.012   93.840    0.012 {cPickle.loads}\n",
      "       27   85.268    3.158   85.268    3.158 {method 'sort' of 'numpy.ndarray' objects}\n",
      "       81   41.972    0.518   41.972    0.518 {numpy.core.multiarray.concatenate}\n",
      "      897   13.664    0.015   13.664    0.015 {cPickle.dumps}\n",
      "       27   12.674    0.469   12.674    0.469 {method 'flatten' of 'numpy.ndarray' objects}\n",
      "       27   12.416    0.460  123.154    4.561 arraysetops.py:96(unique)\n",
      "       27    5.822    0.216    5.822    0.216 {method 'nonzero' of 'numpy.ndarray' objects}\n",
      "       27    3.274    0.121  738.430   27.349 spark_fof.py:363(count_groups_partition)\n",
      "       27    3.115    0.115  741.545   27.465 spark_fof.py:386(<lambda>)\n",
      "       27    2.968    0.110    2.968    0.110 function_base.py:1507(diff)\n",
      "     7546    2.488    0.000    2.488    0.000 {numpy.core.multiarray.where}\n",
      "       27    2.011    0.074  368.488   13.648 serializers.py:259(dump_stream)\n",
      "  2185612    1.605    0.000    3.162    0.000 spark_fof.py:394(<lambda>)\n",
      "  2185666    1.221    0.000    1.557    0.000 broadcast.py:92(value)\n",
      "  2185666    0.336    0.000    0.336    0.000 {hasattr}\n",
      "     7573    0.243    0.000  345.370    0.046 spark_fof.py:338(remap_partition)\n",
      "     7573    0.226    0.000  195.854    0.026 serializers.py:155(_read_with_length)\n",
      "     1794    0.179    0.000    0.179    0.000 {method 'write' of 'file' objects}\n",
      "     7546    0.170    0.000    0.170    0.000 stringsource:341(__cinit__)\n",
      "     7546    0.118    0.000  149.172    0.020 {spark_fof.spark_fof_c.remap_gid_partition_cython}\n",
      "     7573    0.101    0.000  195.955    0.026 serializers.py:136(load_stream)\n",
      "     7573    0.077    0.000    0.434    0.000 serializers.py:542(read_int)\n",
      "     7546    0.064    0.000  148.816    0.020 spark_fof_c.pyx:84(remap_gid_partition_cython (wrapper))\n",
      "     7546    0.058    0.000    0.228    0.000 stringsource:643(memoryview_cwrapper)\n",
      "     7546    0.055    0.000   93.895    0.012 serializers.py:421(loads)\n",
      "     7573    0.027    0.000    0.027    0.000 {_struct.unpack}\n",
      "     9367    0.023    0.000    0.023    0.000 {len}\n",
      "     7546    0.012    0.000    0.012    0.000 stringsource:368(__dealloc__)\n",
      "     7546    0.010    0.000    0.010    0.000 stringsource:649(memoryview_check)\n",
      "      897    0.005    0.000    0.016    0.000 serializers.py:549(write_int)\n",
      "      897    0.005    0.000    0.005    0.000 {_struct.pack}\n",
      "      897    0.005    0.000   13.669    0.015 serializers.py:414(dumps)\n",
      "       54    0.001    0.000    0.001    0.000 numeric.py:484(asanyarray)\n",
      "       27    0.001    0.000    5.823    0.216 fromnumeric.py:1490(nonzero)\n",
      "    81/27    0.000    0.000  741.546   27.465 rdd.py:2370(pipeline_func)\n",
      "       27    0.000    0.000 1110.035   41.112 worker.py:165(process)\n",
      "       81    0.000    0.000  741.546    9.155 rdd.py:316(func)\n",
      "       27    0.000    0.000    0.000    0.000 rdd.py:366(func)\n",
      "       27    0.000    0.000    0.000    0.000 serializers.py:217(load_stream)\n",
      "       27    0.000    0.000    0.000    0.000 spark_fof.py:347(<lambda>)\n",
      "       54    0.000    0.000    0.000    0.000 {numpy.core.multiarray.array}\n",
      "       27    0.000    0.000    0.000    0.000 rdd.py:288(func)\n",
      "       27    0.000    0.000    0.000    0.000 serializers.py:220(_load_stream_without_unbatching)\n",
      "       27    0.000    0.000    0.000    0.000 {iter}\n",
      "       27    0.000    0.000    0.000    0.000 {built-in method from_iterable}\n",
      "       27    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "\n",
      "\n",
      "============================================================\n",
      "Profile of RDD<id=194>\n",
      "============================================================\n",
      "         2098600 function calls in 2.864 seconds\n",
      "\n",
      "   Ordered by: internal time, cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "    26668    1.142    0.000    1.142    0.000 {cPickle.loads}\n",
      "       27    0.679    0.025    2.204    0.082 shuffle.py:263(mergeCombiners)\n",
      "       27    0.232    0.009    0.539    0.020 serializers.py:259(dump_stream)\n",
      "   843108    0.217    0.000    0.217    0.000 spark_fof.py:391(<lambda>)\n",
      "       27    0.106    0.004    0.106    0.004 {method 'items' of 'dict' objects}\n",
      "      243    0.087    0.000    0.087    0.000 {cPickle.dumps}\n",
      "    53363    0.086    0.000    0.086    0.000 {method 'read' of 'file' objects}\n",
      "   926081    0.074    0.000    0.074    0.000 shuffle.py:257(_object_size)\n",
      "    26695    0.053    0.000    1.357    0.000 serializers.py:155(_read_with_length)\n",
      "    26695    0.040    0.000    0.106    0.000 serializers.py:542(read_int)\n",
      "    26695    0.037    0.000    1.394    0.000 serializers.py:136(load_stream)\n",
      "    82973    0.027    0.000    0.027    0.000 spark_fof.py:390(<lambda>)\n",
      "    26695    0.017    0.000    0.017    0.000 {_struct.unpack}\n",
      "    26668    0.015    0.000    1.157    0.000 serializers.py:421(loads)\n",
      "       54    0.012    0.000    2.325    0.043 rdd.py:316(func)\n",
      "      270    0.012    0.000    0.026    0.000 _pslinux.py:1079(memory_info)\n",
      "      270    0.007    0.000    0.007    0.000 {open}\n",
      "      270    0.005    0.000    0.005    0.000 {method 'readline' of 'file' objects}\n",
      "    27181    0.005    0.000    0.005    0.000 {len}\n",
      "      270    0.002    0.000    0.029    0.000 shuffle.py:37(get_used_memory)\n",
      "      486    0.001    0.000    0.001    0.000 {method 'write' of 'file' objects}\n",
      "      270    0.001    0.000    0.026    0.000 _pslinux.py:926(wrapper)\n",
      "      270    0.001    0.000    0.027    0.000 __init__.py:985(memory_info)\n",
      "      243    0.001    0.000    0.002    0.000 serializers.py:549(write_int)\n",
      "      270    0.001    0.000    0.001    0.000 <string>:8(__new__)\n",
      "      297    0.001    0.000    0.001    0.000 {method 'split' of 'str' objects}\n",
      "      243    0.000    0.000    0.000    0.000 {_struct.pack}\n",
      "      270    0.000    0.000    0.007    0.000 _pslinux.py:140(open_binary)\n",
      "      243    0.000    0.000    0.088    0.000 serializers.py:414(dumps)\n",
      "       27    0.000    0.000    0.001    0.000 shuffle.py:69(_get_local_dirs)\n",
      "       27    0.000    0.000    0.002    0.000 shuffle.py:196(__init__)\n",
      "       27    0.000    0.000    0.107    0.004 shuffle.py:337(items)\n",
      "       27    0.000    0.000    2.312    0.086 rdd.py:1798(_mergeCombiners)\n",
      "       27    0.000    0.000    0.000    0.000 posixpath.py:61(join)\n",
      "       27    0.000    0.000    2.864    0.106 worker.py:165(process)\n",
      "      140    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2ae125b7fda0}\n",
      "      270    0.000    0.000    0.000    0.000 {hasattr}\n",
      "       27    0.000    0.000    0.000    0.000 serializers.py:217(load_stream)\n",
      "       27    0.000    0.000    0.001    0.000 shuffle.py:134(_compressed_serializer)\n",
      "      297    0.000    0.000    0.000    0.000 {posix.getpid}\n",
      "       27    0.000    0.000    0.000    0.000 UserDict.py:91(get)\n",
      "       54    0.000    0.000    0.000    0.000 serializers.py:127(__init__)\n",
      "       27    0.000    0.000    0.000    0.000 serializers.py:481(__init__)\n",
      "       27    0.000    0.000    2.325    0.086 rdd.py:2370(pipeline_func)\n",
      "       80    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2b0747ff7da0}\n",
      "       27    0.000    0.000    0.000    0.000 rdd.py:366(func)\n",
      "       40    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2b95e4166da0}\n",
      "       27    0.000    0.000    0.000    0.000 serializers.py:255(__init__)\n",
      "       27    0.000    0.000    0.000    0.000 UserDict.py:103(__contains__)\n",
      "       81    0.000    0.000    0.000    0.000 {method 'startswith' of 'str' objects}\n",
      "       27    0.000    0.000    0.000    0.000 serializers.py:220(_load_stream_without_unbatching)\n",
      "       54    0.000    0.000    0.000    0.000 {iter}\n",
      "       81    0.000    0.000    0.000    0.000 {method 'endswith' of 'str' objects}\n",
      "       27    0.000    0.000    0.000    0.000 {built-in method from_iterable}\n",
      "       27    0.000    0.000    0.000    0.000 shuffle.py:118(__init__)\n",
      "       27    0.000    0.000    0.000    0.000 UserDict.py:35(__getitem__)\n",
      "       27    0.000    0.000    0.000    0.000 serializers.py:190(__init__)\n",
      "       54    0.000    0.000    0.000    0.000 {id}\n",
      "       10    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x2b99d2bf8da0}\n",
      "       27    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "       27    0.000    0.000    0.000    0.000 {isinstance}\n",
      "\n",
      "\n",
      "============================================================\n",
      "Profile of RDD<id=196>\n",
      "============================================================\n",
      "         212469 function calls (212361 primitive calls) in 907.562 seconds\n",
      "\n",
      "   Ordered by: internal time, cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "     7546  348.094    0.046  348.107    0.046 spark_fof_c.pyx:258(relabel_groups)\n",
      "     7546  224.690    0.030  227.243    0.030 spark_fof.py:383(<lambda>)\n",
      "     7546  157.663    0.021  157.663    0.021 spark_fof_c.pyx:84(remap_gid_partition_cython)\n",
      "    15119   99.884    0.007   99.884    0.007 {method 'read' of 'file' objects}\n",
      "     7546   65.343    0.009   65.343    0.009 {cPickle.loads}\n",
      "        4    7.397    1.849    7.397    1.849 {cPickle.load}\n",
      "     7546    2.552    0.000    2.552    0.000 {numpy.core.multiarray.where}\n",
      "    15092    0.336    0.000    0.336    0.000 stringsource:341(__cinit__)\n",
      "     7573    0.296    0.000  900.098    0.119 rdd.py:1008(<genexpr>)\n",
      "     7573    0.240    0.000  165.629    0.022 serializers.py:155(_read_with_length)\n",
      "     7573    0.228    0.000  324.032    0.043 spark_fof.py:338(remap_partition)\n",
      "     7546    0.116    0.000  158.090    0.021 {spark_fof.spark_fof_c.remap_gid_partition_cython}\n",
      "     7546    0.097    0.000  348.411    0.046 {spark_fof.spark_fof_c.relabel_groups}\n",
      "    15092    0.087    0.000    0.423    0.000 stringsource:643(memoryview_cwrapper)\n",
      "     7573    0.085    0.000  165.713    0.022 serializers.py:136(load_stream)\n",
      "     7546    0.067    0.000  157.742    0.021 spark_fof_c.pyx:84(remap_gid_partition_cython (wrapper))\n",
      "     7546    0.064    0.000  348.475    0.046 spark_fof.py:369(relabel_groups_wrapper)\n",
      "     7573    0.064    0.000    0.293    0.000 serializers.py:542(read_int)\n",
      "       54    0.061    0.001  900.159   16.670 {sum}\n",
      "     7546    0.052    0.000  348.528    0.046 spark_fof.py:404(<lambda>)\n",
      "     7546    0.051    0.000   65.394    0.009 serializers.py:421(loads)\n",
      "     7573    0.026    0.000    0.026    0.000 {_struct.unpack}\n",
      "    15092    0.025    0.000    0.025    0.000 stringsource:368(__dealloc__)\n",
      "     7600    0.021    0.000    0.021    0.000 {len}\n",
      "    15092    0.016    0.000    0.016    0.000 stringsource:649(memoryview_check)\n",
      "       27    0.001    0.000    0.002    0.000 serializers.py:259(dump_stream)\n",
      "   135/27    0.000    0.000  907.559   33.613 rdd.py:2370(pipeline_func)\n",
      "       27    0.000    0.000    0.000    0.000 {cPickle.dumps}\n",
      "       27    0.000    0.000    0.000    0.000 serializers.py:217(load_stream)\n",
      "       27    0.000    0.000  907.562   33.613 worker.py:165(process)\n",
      "        4    0.000    0.000    0.000    0.000 {open}\n",
      "      108    0.000    0.000  907.558    8.403 rdd.py:316(func)\n",
      "        4    0.000    0.000    7.398    1.849 broadcast.py:82(load)\n",
      "       27    0.000    0.000    0.000    0.000 {operator.add}\n",
      "       27    0.000    0.000    7.398    0.274 spark_fof.py:347(<lambda>)\n",
      "       54    0.000    0.000    0.000    0.000 {method 'write' of 'file' objects}\n",
      "       27    0.000    0.000  900.159   33.339 rdd.py:1008(<lambda>)\n",
      "       27    0.000    0.000    7.398    0.274 broadcast.py:92(value)\n",
      "       27    0.000    0.000    0.000    0.000 serializers.py:549(write_int)\n",
      "       54    0.000    0.000    0.000    0.000 rdd.py:288(func)\n",
      "       27    0.000    0.000    0.000    0.000 serializers.py:414(dumps)\n",
      "       54    0.000    0.000    0.000    0.000 rdd.py:865(func)\n",
      "       27    0.000    0.000    0.000    0.000 {_struct.pack}\n",
      "       27    0.000    0.000    0.000    0.000 serializers.py:220(_load_stream_without_unbatching)\n",
      "       27    0.000    0.000    0.000    0.000 {hasattr}\n",
      "       27    0.000    0.000    0.000    0.000 rdd.py:999(<lambda>)\n",
      "       27    0.000    0.000    0.000    0.000 {built-in method from_iterable}\n",
      "       27    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "       27    0.000    0.000    0.000    0.000 {iter}\n",
      "        4    0.000    0.000    0.000    0.000 {gc.enable}\n",
      "        4    0.000    0.000    0.000    0.000 {gc.disable}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc.show_profiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
